{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a34a55f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shivam here\n",
      "Requirement already satisfied: python-dotenv in /Users/shivamkumar/Documents/Krish-Naik-Course-Agentic AI/agentic-ai-course/venv/lib/python3.11/site-packages (1.1.0)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.1.2\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.1.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "print(\"shivam here\")\n",
    "!pip install python-dotenv\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5ceb64c2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "# Load the .env file\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e15982e",
   "metadata": {},
   "source": [
    "# Config the Model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "30386c61",
   "metadata": {},
   "outputs": [],
   "source": [
    "GEMINI_API_KEY = os.getenv(\"GEMINI_API_KEY\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5004b8db",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello there! How can I help you today?\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from langchain_google_genai import GoogleGenerativeAI\n",
    "model = GoogleGenerativeAI(\n",
    "    model=\"gemini-1.5-flash\",\n",
    "    temperature=0.2,\n",
    "    google_api_key=GEMINI_API_KEY,\n",
    "    max_output_tokens=1024,)\n",
    "response = model.invoke(\"hello gemini\")\n",
    "print(response)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "526fd8f0",
   "metadata": {},
   "source": [
    "# Config the embedding model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "19ee3987",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "384"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "model_name = \"BAAI/bge-small-en\"\n",
    "embeddings = HuggingFaceEmbeddings(\n",
    "    model_name=model_name)\n",
    "len(embeddings.embed_query(\"hello world\"))  # [768]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ae3594c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.1.2\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.1.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install -qU pypdf"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b47f7f43",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "a8ef6d59",
   "metadata": {},
   "source": [
    "# Lets store the data in vector DB "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "dfa8145a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.document_loaders import TextLoader, DirectoryLoader\n",
    "from langchain_community.vectorstores import Chroma\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.document_loaders import PyPDFLoader\n",
    "\n",
    "loader = DirectoryLoader(\"/Users/shivamkumar/Documents/Krish-Naik-Course-Agentic AI/agentic-ai-course/data/colbert.pdf\", glob=\"**/*.pdf\", loader_cls=PyPDFLoader)\n",
    "# loader = TextLoader(\"data/colbert.pdf\")\n",
    "loader = PyPDFLoader(\"/Users/shivamkumar/Documents/Krish-Naik-Course-Agentic AI/agentic-ai-course/data/colbert.pdf\")\n",
    "documents = loader.load()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9b22afc4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 78 documents\n"
     ]
    }
   ],
   "source": [
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)\n",
    "texts = text_splitter.split_documents(documents)\n",
    "print(f\"Loaded {len(texts)} documents\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0b1c3e1b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'producer': 'macOS Version 13.5 (Build 22G74) Quartz PDFContext, AppendMode 1.1', 'creator': 'LaTeX with hyperref package', 'creationdate': \"D:20200605003220Z00'00'\", 'author': 'Omar Khattab and Matei Zaharia', 'subject': '', 'trapped': '/False', 'title': 'ColBERT: Efficient and Effective Passage Search via Contextualized Late Interaction over BERT', 'moddate': \"D:20241127020339Z00'00'\", 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'keywords': '', 'source': '/Users/shivamkumar/Documents/Krish-Naik-Course-Agentic AI/agentic-ai-course/data/colbert.pdf', 'total_pages': 10, 'page': 0, 'page_label': '1'}, page_content='ColBERT: Eﬀicient and Eﬀective Passage Search via\\nContextualized Late Interaction over BERT\\nOmar Kha/t_tab\\nStanford University\\nokha/t_tab@stanford.edu\\nMatei Zaharia\\nStanford University\\nmatei@cs.stanford.edu\\nABSTRACT\\nRecent progress in Natural Language Understanding (NLU) is driv-\\ning fast-paced advances in Information Retrieval (IR), largely owed\\nto /f_ine-tuning deep language models (LMs) for document ranking.\\nWhile remarkably eﬀective, the ranking models based on these LMs\\nincrease computational cost by orders of magnitude over prior ap-\\nproaches, particularly as they must feed each query–document pair\\nthrough a massive neural network to compute a single relevance\\nscore. To tackle this, we present ColBERT, a novel ranking model\\nthat adapts deep LMs (in particular, BERT) for eﬃcient retrieval.\\nColBERT introduces a late interactionarchitecture that indepen-\\ndently encodes the query and the document using BERT and then\\nemploys a cheap yet powerful interaction step that models their\\n/f_ine-grained similarity. By delaying and yet retaining this /f_ine-\\ngranular interaction, ColBERT can leverage the expressiveness of\\ndeep LMs while simultaneously gaining the ability to pre-compute\\ndocument representations oﬄine, considerably speeding up query\\nprocessing. Beyond reducing the cost of re-ranking the documents\\nretrieved by a traditional model, ColBERT’s pruning-friendly in-\\nteraction mechanism enables leveraging vector-similarity indexes\\nfor end-to-end retrieval directly from a large document collection.\\nWe extensively evaluate ColBERT using two recent passage search\\ndatasets. Results show that ColBERT’s eﬀectiveness is competitive\\nwith existing BERT-based models (and outperforms every non-\\nBERT baseline), while executing two orders-of-magnitude faster\\nand requiring four orders-of-magnitude fewer FLOPs per query.\\nACM Reference format:\\nOmar Kha/t_tab and Matei Zaharia. 2020. ColBERT: Eﬃcient and Eﬀective Pas-\\nsage Search via Contextualized Late Interaction over BERT. In Proceedings\\nof Proceedings of the 43rd International ACM SIGIR Conference on Research\\nand Development in Information Retrieval, Virtual Event, China, July 25–30,\\n2020 (SIGIR ’20),10 pages.\\nDOI: 10.1145/3397271.3401075\\n1 INTRODUCTION\\nOver the past few years, the Information Retrieval (IR) community\\nhas witnessed the introduction of a host of neural ranking models,\\nincluding DRMM [7], KNRM [4, 36], and Duet [20, 22]. In contrast\\nPermission to make digital or hard copies of all or part of this work for personal or\\nclassroom use is granted without fee provided that copies are not made or distributed\\nfor pro/f_it or commercial advantage and that copies bear this notice and the full citation\\non the /f_irst page. Copyrights for components of this work owned by others than the\\nauthor(s) must be honored. Abstracting with credit is permi/t_ted. To copy otherwise, or\\nrepublish, to post on servers or to redistribute to lists, requires prior speci/f_ic permission\\nand/or a fee. Request permissions from permissions@acm.org.\\nSIGIR ’20, Virtual Event, China\\n© 2020 Copyright held by the owner/author(s). Publication rights licensed to ACM.\\n978-1-4503-8016-4/20/07. . .$15.00\\nDOI: 10.1145/3397271.3401075\\n0.15 0.20 0.25 0.30 0.35 0.40\\nMRR@10\\n101\\n102\\n103\\n104\\n105\\nQuery Latency (ms)\\nBM25 doc2queryKNRM\\nDuet\\nDeepCT\\nfT+ConvKNRM\\ndocTTTTTquery\\nBERT-base\\nBERT-large\\nColBERT (re-rank)\\nColBERT (full retrieval)\\nBag-of-Words (BoW) Model\\nBoW Model with NLU Augmentation\\nNeural Matching Model\\nDeep Language Model\\nColBERT (ours)\\nFigure 1: Eﬀectiveness (MRR@10) versus Mean /Q_uery La-\\ntency (log-scale) for a number of representative ranking\\nmodels on MS MARCO Ranking [24]. /T_he /f_igure also shows\\nColBERT. Neural re-rankers run on top of the oﬃcial BM25\\ntop-1000 results and use a Tesla V100 GPU. Methodology and\\ndetailed results are in§4.\\nto prior learning-to-rank methods that rely on hand-cra/f_ted fea-\\ntures, these models employ embedding-based representations of\\nqueries and documents and directly model local interactions(i.e.,\\n/f_ine-granular relationships) between their contents. Among them,\\na recent approach has emerged that /f_ine-tunesdeep pre-trained\\nlanguage models (LMs) like ELMo [29] and BERT [5] for estimating\\nrelevance. By computing deeply-contextualized semantic repre-\\nsentations of query–document pairs, these LMs help bridge the\\npervasive vocabulary mismatch [21, 42] between documents and\\nqueries [30]. Indeed, in the span of just a few months, a number\\nof ranking models based on BERT have achieved state-of-the-art\\nresults on various retrieval benchmarks [ 3, 18, 25, 39] and have\\nbeen proprietarily adapted for deployment by Google1 and Bing2.\\nHowever, the remarkable gains delivered by these LMs come\\nat a steep increase in computational cost. Hofst¨a/t_teret al.[9] and\\nMacAvaney et al.[18] observe that BERT-based models in the lit-\\nerature are 100-1000×more computationally expensive than prior\\nmodels—some of which are arguably not inexpensive to begin with\\n[13]. /T_his quality–cost tradeoﬀ is summarized by Figure 1, which\\ncompares two BERT-based rankers [25, 27] against a representative\\nset of ranking models. /T_he /f_igure uses MS MARCO Ranking [24],\\na recent collection of 9M passages and 1M queries from Bing’s\\nlogs. It reports retrieval eﬀectiveness (MRR@10) on the oﬃcial\\nvalidation set as well as average query latency (log-scale) using a\\nhigh-end server that dedicates one Tesla V100 GPU per query for\\nneural re-rankers. Following the re-ranking setup of MS MARCO,\\nColBERT (re-rank), the Neural Matching Models, and the Deep LMs\\nre-rank the MS MARCO’s oﬃcial top-1000 documents per query.\\n1h/t_tps://blog.google/products/search/search-language-understanding-bert/\\n2h/t_tps://azure.microso/f_t.com/en-us/blog/bing-delivers-its-largest-improvement-\\nin-search-experience-using-azure-gpus/\\narXiv:2004.12832v2  [cs.IR]  4 Jun 2020'),\n",
       " Document(metadata={'producer': 'macOS Version 13.5 (Build 22G74) Quartz PDFContext, AppendMode 1.1', 'creator': 'LaTeX with hyperref package', 'creationdate': \"D:20200605003220Z00'00'\", 'author': 'Omar Khattab and Matei Zaharia', 'subject': '', 'trapped': '/False', 'title': 'ColBERT: Efficient and Effective Passage Search via Contextualized Late Interaction over BERT', 'moddate': \"D:20241127020339Z00'00'\", 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'keywords': '', 'source': '/Users/shivamkumar/Documents/Krish-Naik-Course-Agentic AI/agentic-ai-course/data/colbert.pdf', 'total_pages': 10, 'page': 1, 'page_label': '2'}, page_content='QueryDocument\\nMaxSim∑MaxSimMaxSimsQueryCNN  /  Match KernelsCNN  /  Match Kernels / MLPMLPs\\nDocument(c) All-to-all Interaction(e.g., BERT)(b) Query-Document Interaction(e.g., DRMM, KNRM, Conv-KNRM)(d) Late Interaction(i.e., the proposed ColBERT)(a) Representation-based Similarity(e.g., DSSM, SNRM)QueryDocument\\nsQueryDocument\\ns\\nFigure 2: Schematic diagrams illustrating query–document matching paradigms in neural IR. /T_he /f_igure contrasts existing\\napproaches (sub-/f_igures (a), (b), and (c)) with the proposed late interaction paradigm (sub-/f_igure (d)).\\nOther methods, including ColBERT (full retrieval), directly retrieve\\nthe top-1000 results from the entire collection.\\nAs the /f_igure shows, BERT considerably improves search preci-\\nsion, raising MRR@10 by almost 7% against the best previous meth-\\nods; simultaneously, it increases latency by up to tens of thousands\\nof milliseconds even with a high-end GPU. /T_his poses a challenging\\ntradeoﬀ since raising query response times by as li/t_tle as 100ms is\\nknown to impact user experience and even measurably diminish\\nrevenue [17]. To tackle this problem, recent work has started ex-\\nploring using Natural Language Understanding (NLU) techniques\\nto augment traditional retrieval models like BM25 [32]. For exam-\\nple, Nogueira et al.[26, 28] expand documents with NLU-generated\\nqueries before indexing with BM25 scores and Dai & Callan [2] re-\\nplace BM25’s term frequency with NLU-estimated term importance.\\nDespite successfully reducing latency, these approaches generally\\nreduce precision substantially relative to BERT.\\nTo reconcile eﬃciency and contextualization in IR, we propose\\nColBERT, a ranking model based on contextualized late interac-\\ntion over BERT. As the name suggests, ColBERT proposes a novel\\nlate interactionparadigm for estimating relevance between a query\\nq and a document d. Under late interaction, q and d are separately\\nencoded into two sets of contextual embeddings, and relevance is\\nevaluated using cheap and pruning-friendly computations between\\nboth sets—that is, fast computations that enable ranking without\\nexhaustively evaluating every possible candidate.\\nFigure 2 contrasts our proposed late interaction approach with\\nexisting neural matching paradigms. On the le/f_t, Figure 2 (a) illus-\\ntrates representation-focused rankers, which independently compute\\nan embedding for q and another for d and estimate relevance as\\na single similarity score between two vectors [12, 41]. Moving to\\nthe right, Figure 2 (b) visualizes typical interaction-focused rankers.\\nInstead of summarizing q and d into individual embeddings, these\\nrankers model word- and phrase-level relationships across q and d\\nand match them using a deep neural network (e.g., with CNNs/MLPs\\n[22] or kernels [36]). In the simplest case, they feed the neural net-\\nwork an interaction matrix that re/f_lects the similiarity between\\nevery pair of words across q and d. Further right, Figure 2 (c) illus-\\ntrates a more powerful interaction-based paradigm, which models\\nthe interactions between words within as well as across q and d at\\nthe same time, as in BERT’s transformer architecture [25].\\n/T_hese increasingly expressive architectures are in tension. While\\ninteraction-based models (i.e., Figure 2 (b) and (c)) tend to be su-\\nperior for IR tasks [8, 21], a representation-focused model—by iso-\\nlating the computations among q and d—makes it possible to pre-\\ncompute document representations oﬄine [ 41], greatly reducing\\nthe computational load per query. In this work, we observe that\\nthe /f_ine-grained matching of interaction-based models and the pre-\\ncomputation of document representations of representation-based\\nmodels can be combined by retaining yet judiciously delaying the\\nquery–document interaction. Figure 2 (d) illustrates an architec-\\nture that precisely does so. As illustrated, every query embedding\\ninteracts with all document embeddings via a MaxSim operator,\\nwhich computes maximum similarity (e.g., cosine similarity), and\\nthe scalar outputs of these operators are summed across query\\nterms. /T_his paradigm allows ColBERT to exploit deep LM-based\\nrepresentations while shi/f_ting the cost of encoding documents of-\\n/f_line and amortizing the cost of encoding the query once across\\nall ranked documents. Additionally, it enables ColBERT to lever-\\nage vector-similarity search indexes (e.g., [ 1, 15]) to retrieve the\\ntop-k results directly from a large document collection, substan-\\ntially improving recall over models that only re-rank the output of\\nterm-based retrieval.\\nAs Figure 1 illustrates, ColBERT can serve queries in tens or\\nfew hundreds of milliseconds. For instance, when used for re-\\nranking as in “ColBERT (re-rank)”, it delivers over 170×speedup\\n(and requires 14,000×fewer FLOPs) relative to existing BERT-based\\nmodels, while being more eﬀective than every non-BERT baseline\\n(§4.2 & 4.3). ColBERT’s indexing—the only time it needs to feed\\ndocuments through BERT—is also practical: it can index the MS\\nMARCO collection of 9M passages in about 3 hours using a single\\nserver with four GPUs (§4.5), retaining its eﬀectiveness with a space\\nfootprint of as li/t_tle as few tens of GiBs. Our extensive ablation\\nstudy (§4.4) shows that late interaction, its implementation via\\nMaxSim operations, and crucial design choices within our BERT-\\nbased encoders are all essential to ColBERT’s eﬀectiveness.\\nOur main contributions are as follows.\\n(1) We proposelate interaction(§3.1) as a paradigm for eﬃcient\\nand eﬀective neural ranking.\\n(2) We present ColBERT (§3.2 & 3.3), a highly-eﬀective model\\nthat employs novel BERT-based query and document en-\\ncoders within the late interaction paradigm.'),\n",
       " Document(metadata={'producer': 'macOS Version 13.5 (Build 22G74) Quartz PDFContext, AppendMode 1.1', 'creator': 'LaTeX with hyperref package', 'creationdate': \"D:20200605003220Z00'00'\", 'author': 'Omar Khattab and Matei Zaharia', 'subject': '', 'trapped': '/False', 'title': 'ColBERT: Efficient and Effective Passage Search via Contextualized Late Interaction over BERT', 'moddate': \"D:20241127020339Z00'00'\", 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'keywords': '', 'source': '/Users/shivamkumar/Documents/Krish-Naik-Course-Agentic AI/agentic-ai-course/data/colbert.pdf', 'total_pages': 10, 'page': 2, 'page_label': '3'}, page_content='(3) We show how to leverage ColBERT both for re-ranking on\\ntop of a term-based retrieval model (§3.5) and for searching\\na full collection using vector similarity indexes (§3.6).\\n(4) We evaluate ColBERT on MS MARCO and TREC CAR, two\\nrecent passage search collections.\\n2 RELATED WORK\\nNeural Matching Models.Over the past few years, IR researchers\\nhave introduced numerous neural architectures for ranking. In\\nthis work, we compare against KNRM [4, 36], Duet [20, 22], Con-\\nvKNRM [4], and fastText+ConvKNRM [ 10]. KNRM proposes a\\ndiﬀerentiable kernel-pooling technique for extracting matching\\nsignals from an interaction matrix, while Duet combines signals\\nfrom exact-match-based as well as embedding-based similarities\\nfor ranking. Introduced in 2018, ConvKNRM learns to match n-\\ngrams in the query and the document. Lastly, fastText+ConvKNRM\\n(abbreviated fT+ConvKNRM) tackles the absence of rare words\\nfrom typical word embeddings lists by adopting sub-word token\\nembeddings.\\nIn 2018, Zamani et al.[41] introduced SNRM, a representation-\\nfocused IR model that encodes each query and each document as\\na single, sparse high-dimensional vector of “latent terms”. By pro-\\nducing a sparse-vector representation for each document, SNRM\\nis able to use a traditional IR inverted index for representing docu-\\nments, allowing fast end-to-end retrieval. Despite highly promising\\nresults and insights, SNRM’s eﬀectiveness is substantially outper-\\nformed by the state of the art on the datasets with which it was\\nevaluated (e.g., see [18, 38]). While SNRM employs sparsity to al-\\nlow using inverted indexes, we relax this assumption and compare\\na (dense) BERT-based representation-focused model against our\\nlate-interaction ColBERT in our ablation experiments in §4.4. For a\\ndetailed overview of existing neural ranking models, we refer the\\nreaders to two recent surveys of the literature [8, 21].\\nLanguage Model Pretraining for IR.Recent work in NLU\\nemphasizes the importance pre-training language representation\\nmodels in an unsupervised fashion before subsequently /f_ine-tuning\\nthem on downstream tasks. A notable example is BERT [5], a bi-\\ndirectional transformer-based language model whose /f_ine-tuning\\nadvanced the state of the art on various NLU benchmarks. Nogueiraet\\nal. [25], MacAvaney et al.[18], and Dai & Callan [ 3] investigate\\nincorporating such LMs (mainly BERT, but also ELMo [29]) on dif-\\nferent ranking datasets. As illustrated in Figure 2 (c), the common\\napproach (and the one adopted by Nogueira et al.on MS MARCO\\nand TREC CAR) is to feed the query–document pair through BERT\\nand use an MLP on top of BERT’s [CLS] output token to produce a\\nrelevance score. Subsequent work by Nogueiraet al.[27] introduced\\nduoBERT, which /f_ine-tunes BERT to compare the relevance of a\\npair of documents given a query. Relative to their single-document\\nBERT, this gives duoBERT a 1% MRR@10 advantage on MS MARCO\\nwhile increasing the cost by at least 1.4×.\\nBERT Optimizations. As discussed in §1, these LM-based\\nrankers can be highly expensive in practice. While ongoing ef-\\nforts in the NLU literature for distilling [14, 33], compressing [40],\\nand pruning [19] BERT can be instrumental in narrowing this gap,\\nQueryDocument\\nQuery Encoder, fQDocument Encoder, fDMaxSimMaxSimMaxSimscoreOffline Indexing\\nFigure 3: /T_he general architecture of ColBERT given a query\\nq and a documentd.\\nthey generally achieve signi/f_icantly smaller speedups than our re-\\ndesigned architecture for IR, due to their generic nature, and more\\naggressive optimizations o/f_ten come at the cost of lower quality.\\nEﬃcient NLU-based Models. Recently, a direction emerged\\nthat employs expensive NLU computation oﬄine. /T_his includes\\ndoc2query [28] and DeepCT [ 2]. /T_he doc2query model expands\\neach document with a pre-de/f_ined number of synthetic queries\\nqueries generated by a seq2seq transformer model that is trained to\\ngenerate queries given a document. It then relies on a BM25 index\\nfor retrieval from the (expanded) documents. DeepCT uses BERT\\nto produce the term frequencycomponent of BM25 in a context-\\naware manner, essentially representing a feasible realization of the\\nterm-independence assumption with neural networks [23]. Lastly,\\ndocTTTTTquery [ 26] is identical to doc2query except that it /f_ine-\\ntunes a pre-trained model (namely, T5 [ 31]) for generating the\\npredicted queries.\\nConcurrently with our dra/f_ting of this paper, Hofst¨a/t_teret al.[11]\\npublished their Transformer-Kernel (TK) model. At a high level, TK\\nimproves the KNRM architecture described earlier: while KNRM\\nemploys kernel pooling on top of word-embedding-based inter-\\naction, TK uses a Transformer [ 34] component for contextually\\nencoding queries and documents before kernel pooling. TK estab-\\nlishes a new state-of-the-art for non-BERT models on MS MARCO\\n(Dev); however, the best non-ensemble MRR@10 it achieves is 31%\\nwhile ColBERT reaches up to 36%. Moreover, due to indexing docu-\\nment representations oﬄine and employing a MaxSim-based late\\ninteraction mechanism, ColBERT is much more scalable, enabling\\nend-to-end retrieval which is not supported by TK.\\n3 COLBERT\\nColBERT prescribes a simple framework for balancing the quality\\nand cost of neural IR, particularly deep language models like BERT.\\nAs introduced earlier, delaying the query–document interaction can\\nfacilitate cheap neural re-ranking (i.e., through pre-computation)\\nand even support practical end-to-end neural retrieval (i.e., through\\npruning via vector-similarity search). ColBERT addresses how to\\ndo so while still preserving the eﬀectiveness of state-of-the-art\\nmodels, which condition the bulk of their computations on the\\njoint query–document pair.'),\n",
       " Document(metadata={'producer': 'macOS Version 13.5 (Build 22G74) Quartz PDFContext, AppendMode 1.1', 'creator': 'LaTeX with hyperref package', 'creationdate': \"D:20200605003220Z00'00'\", 'author': 'Omar Khattab and Matei Zaharia', 'subject': '', 'trapped': '/False', 'title': 'ColBERT: Efficient and Effective Passage Search via Contextualized Late Interaction over BERT', 'moddate': \"D:20241127020339Z00'00'\", 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'keywords': '', 'source': '/Users/shivamkumar/Documents/Krish-Naik-Course-Agentic AI/agentic-ai-course/data/colbert.pdf', 'total_pages': 10, 'page': 3, 'page_label': '4'}, page_content='Even though ColBERT’s late-interaction framework can be ap-\\nplied to a wide variety of architectures (e.g., CNNs, RNNs, trans-\\nformers, etc.), we choose to focus this work on bi-directional transformer-\\nbased encoders (i.e., BERT) owing to their state-of-the-art eﬀective-\\nness yet very high computational cost.\\n3.1 Architecture\\nFigure 3 depicts the general architecture of ColBERT, which com-\\nprises: (a) a query encoder fQ , (b) a document encoder fD , and (c)\\nthe late interaction mechanism. Given a query q and document d,\\nfQ encodes q into a bag of /f_ixed-size embeddingsEq while fD en-\\ncodes d into another bag Ed . Crucially, each embeddings in Eq and\\nEd is contextualized based on the other terms inq or d, respectively.\\nWe describe our BERT-based encoders in §3.2.\\nUsing Eq and Ed , ColBERT computes the relevance score be-\\ntween q and d via late interaction, which we de/f_ine as a summation\\nof maximum similarity (MaxSim) operators. In particular, we /f_ind\\nthe maximum cosine similarity of each /v.alt∈Eq with vectors in Ed ,\\nand combine the outputs via summation. Besides cosine, we also\\nevaluate squared L2 distance as a measure of vector similarity. In-\\ntuitively, this interaction mechanism so/f_tlysearches for each query\\nterm tq—in a manner that re/f_lects its context in the query—against\\nthe document’s embeddings, quantifying the strength of the “match”\\nvia the largest similarity score between tq and a document term td .\\nGiven these term scores, it then estimates the document relevance\\nby summing the matching evidence across all query terms.\\nWhile more sophisticated matching is possible with other choices\\nsuch as deep convolution and a/t_tention layers (i.e., as in typical\\ninteraction-focused models), a summation of maximum similarity\\ncomputations has two distinctive characteristics. First, it stands\\nout as a particularly cheap interaction mechanism, as we examine\\nits FLOPs in §4.2. Second, and more importantly, it is amenable\\nto highly-eﬃcient pruning for top- k retrieval, as we evaluate in\\n§4.3. /T_his enables using vector-similarity algorithms for skipping\\ndocuments without materializing the full interaction matrix or even\\nconsidering each document in isolation. Other cheap choices (e.g.,\\na summation of average similarity scores, instead of maximum) are\\npossible; however, many are less amenable to pruning. In §4.4, we\\nconduct an extensive ablation study that empirically veri/f_ies the ad-\\nvantage of our MaxSim-based late interaction against alternatives.\\n3.2 /Q_uery & Document Encoders\\nPrior to late interaction, ColBERT encodes each query or document\\ninto a bag of embeddings, employing BERT-based encoders. We\\nshare a single BERT model among our query and document en-\\ncoders but distinguish input sequences that correspond to queries\\nand documents by prepending a special token [Q] to queries and\\nanother token [D] to documents.\\n/Q_uery Encoder.Given a textual query q, we tokenize it into its\\nBERT-based WordPiece [35] tokensq1q2...ql . We prepend the token\\n[Q] to the query. We place this token right a/f_ter BERT’s sequence-\\nstart token [CLS]. If the query has fewer than a pre-de/f_ined number\\nof tokens Nq, we pad it with BERT’s special [mask] tokens up\\nto length Nq (otherwise, we truncate it to the /f_irst Nq tokens).\\n/T_his padded sequence of input tokens is then passed into BERT’s\\ndeep transformer architecture, which computes a contextualized\\nrepresentation of each token.\\nWe denote the padding with masked tokens as query augmen-\\ntation, a step that allows BERT to produce query-based embeddings\\nat the positions corresponding to these masks. /Q_uery augmentation\\nis intended to serve as a so/f_t, diﬀerentiable mechanism for learning\\nto expand queries with new terms or to re-weigh existing terms\\nbased on their importance for matching the query. As we show in\\n§4.4, this operation is essential for ColBERT’s eﬀectiveness.\\nGiven BERT’s representation of each token, our encoder passes\\nthe contextualized output representations through a linear layer\\nwith no activations. /T_his layer serves to control the dimension\\nof ColBERT’s embeddings, producingm-dimensional embeddings\\nfor the layer’s output sizem. As we discuss later in more detail,\\nwe typically /f_ixm to be much smaller than BERT’s /f_ixed hidden\\ndimension.\\nWhile ColBERT’s embedding dimension has limited impact on\\nthe eﬃciency of query encoding, this step is crucial for controlling\\nthe space footprint of documents, as we show in§4.5. In addition, it\\ncan have a signi/f_icant impact on query execution time, particularly\\nthe time taken for transferring the document representations onto\\nthe GPU from system memory (where they reside before processing\\na query). In fact, as we show in §4.2, gathering, stacking, and\\ntransferring the embeddings from CPU to GPU can be the most\\nexpensive step in re-ranking with ColBERT. Finally, the output\\nembeddings are normalized so each has L2 norm equal to one.\\n/T_he result is that the dot-product of any two embeddings becomes\\nequivalent to their cosine similarity, falling in the [−1, 1]range.\\nDocument Encoder.Our document encoder has a very similar\\narchitecture. We /f_irst segment a documentd into its constituent to-\\nkens d1d2...dm, to which we prepend BERT’s start token[CLS] fol-\\nlowed by our special token [D] that indicates a document sequence.\\nUnlike queries, we do not append [mask] tokens to documents. Af-\\nter passing this input sequence through BERT and the subsequent\\nlinear layer, the document encoder /f_ilters out the embeddings corre-\\nsponding to punctuation symbols, determined via a pre-de/f_ined list.\\n/T_his /f_iltering is meant to reduce the number of embeddings per doc-\\nument, as we hypothesize that (even contextualized) embeddings\\nof punctuation are unnecessary for eﬀectiveness.\\nIn summary, given q = q0q1...ql and d = d0d1...dn, we compute\\nthe bags of embeddings Eq and Ed in the following manner, where\\n# refers to the [mask] tokens:\\nEq := Normalize(CNN(BERT(“[Q]q0q1...ql ##... #”))) (1)\\nEd := Filter(Normalize(CNN(BERT(“[D]d0d1...dn”)))) (2)\\n3.3 Late Interaction\\nGiven the representation of a query q and a document d, the rel-\\nevance score of d to q, denoted as Sq, d , is estimated via late in-\\nteraction between their bags of contextualized embeddings. As\\nmentioned before, this is conducted as a sum of maximum sim-\\nilarity computations, namely cosine similarity (implemented as\\ndot-products due to the embedding normalization) or squared L2\\ndistance.'),\n",
       " Document(metadata={'producer': 'macOS Version 13.5 (Build 22G74) Quartz PDFContext, AppendMode 1.1', 'creator': 'LaTeX with hyperref package', 'creationdate': \"D:20200605003220Z00'00'\", 'author': 'Omar Khattab and Matei Zaharia', 'subject': '', 'trapped': '/False', 'title': 'ColBERT: Efficient and Effective Passage Search via Contextualized Late Interaction over BERT', 'moddate': \"D:20241127020339Z00'00'\", 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'keywords': '', 'source': '/Users/shivamkumar/Documents/Krish-Naik-Course-Agentic AI/agentic-ai-course/data/colbert.pdf', 'total_pages': 10, 'page': 4, 'page_label': '5'}, page_content='Sq, d :=\\nÕ\\ni ∈[|Eq |]\\nmax\\nj ∈[|Ed |]\\nEqi ·ET\\ndj\\n(3)\\nColBERT is diﬀerentiable end-to-end. We /f_ine-tune the BERT\\nencoders and train from scratch the additional parameters (i.e., the\\nlinear layer and the [Q] and [D] markers’ embeddings) using the\\nAdam [16] optimizer. Notice that our interaction mechanism has\\nno trainable parameters. Given a triple ⟨q, d+, d−⟩with query q,\\npositive document d+ and negative document d−, ColBERT is used\\nto produce a score for each document individually and is optimized\\nvia pairwise so/f_tmax cross-entropy loss over the computed scores\\nof d+ and d−.\\n3.4 Oﬀline Indexing: Computing & Storing\\nDocument Embeddings\\nBy design, ColBERT isolates almost all of the computations between\\nqueries and documents, largely to enable pre-computing document\\nrepresentations oﬄine. At a high level, our indexing procedure is\\nstraight-forward: we proceed over the documents in the collection\\nin batches, running our document encoder fD on each batch and\\nstoring the output embeddings per document. Although indexing\\na set of documents is an oﬄine process, we incorporate a few\\nsimple optimizations for enhancing the throughput of indexing. As\\nwe show in §4.5, these optimizations can considerably reduce the\\noﬄine cost of indexing.\\nTo begin with, we exploit multiple GPUs, if available, for faster\\nencoding of batches of documents in parallel. When batching, we\\npad all documents to the maximum length of a document within\\nthe batch.3 To make capping the sequence length on a per-batch\\nbasis more eﬀective, our indexer proceeds through documents in\\ngroups of B (e.g., B = 100,000) documents. It sorts these documents\\nby length and then feeds batches of b (e.g., b = 128) documents of\\ncomparable length through our encoder. /T_his length-based bucket-\\ning is sometimes refered to as a BucketIterator in some libraries\\n(e.g., allenNLP). Lastly, while most computations occur on the GPU,\\nwe found that a non-trivial portion of the indexing time is spent on\\npre-processing the text sequences, primarily BERT’s WordPiece to-\\nkenization. Exploiting that these operations are independent across\\ndocuments in a batch, we parallelize the pre-processing across the\\navailable CPU cores.\\nOnce the document representations are produced, they are saved\\nto disk using 32-bit or 16-bit values to represent each dimension.\\nAs we describe in §3.5 and 3.6, these representations are either\\nsimply loaded from disk for ranking or are subsequently indexed\\nfor vector-similarity search, respectively.\\n3.5 Top- k Re-ranking with ColBERT\\nRecall that ColBERT can be used for re-ranking the output of an-\\nother retrieval model, typically a term-based model, or directly\\nfor end-to-end retrieval from a document collection. In this sec-\\ntion, we discuss how we use ColBERT for ranking a small set of\\nk (e.g., k = 1000) documents given a query q. Since k is small, we\\nrely on batch computations to exhaustively score each document\\n3/T_he public BERT implementations we saw simply pad to a pre-de/f_ined length.\\n(unlike our approach in §3.6). To begin with, our query serving sub-\\nsystem loads the indexed documents representations into memory,\\nrepresenting each document as a matrix of embeddings.\\nGiven a query q, we compute its bag of contextualized embed-\\ndings Eq (Equation 1) and, concurrently, gather the document repre-\\nsentations into a 3-dimensional tensor D consisting of k document\\nmatrices. We pad the k documents to their maximum length to\\nfacilitate batched operations, and move the tensor D to the GPU’s\\nmemory. On the GPU, we compute a batch dot-product of Eq and\\nD, possibly over multiple mini-batches. /T_he output materializes a\\n3-dimensional tensor that is a collection of cross-match matrices\\nbetween q and each document. To compute the score of each docu-\\nment, we reduce its matrix across document terms via a max-pool\\n(i.e., representing an exhaustive implementation of our MaxSim\\ncomputation) and reduce across query terms via a summation. Fi-\\nnally, we sort the k documents by their total scores.\\nRelative to existing neural rankers (especially, but not exclu-\\nsively, BERT-based ones), this computation is very cheap that, in\\nfact, its cost is dominated by the cost of gathering and transferring\\nthe pre-computed embeddings. To illustrate, ranking k documents\\nvia typical BERT rankers requires feeding BERT k diﬀerent inputs\\neach of length l = |q|+ |di |for query q and documents di , where\\na/t_tention has quadratic cost in the length of the sequence. In con-\\ntrast, ColBERT feeds BERT only a single, much shorter sequence of\\nlength l = |q|. Consequently, ColBERT is not only cheaper, it also\\nscales much be/t_ter withk as we examine in §4.2.\\n3.6 End-to-end Top-k Retrieval with ColBERT\\nAs mentioned before, ColBERT’s late-interaction operator is speci/f_i-\\ncally designed to enable end-to-end retrieval from a large collection,\\nlargely to improve recall relative to term-based retrieval approaches.\\n/T_his section is concerned with cases where the number of docu-\\nments to be ranked is too large for exhaustive evaluation of each\\npossible candidate document, particularly when we are only inter-\\nested in the highest scoring ones. Concretely, we focus here on\\nretrieving the top-k results directly from a large document collec-\\ntion with N (e.g., N = 10, 000, 000) documents, where k ≪N .\\nTo do so, we leverage the pruning-friendly nature of the MaxSim\\noperations at the backbone of late interaction. Instead of apply-\\ning MaxSim between one of the query embeddings and all of one\\ndocument’s embeddings, we can use fast vector-similarity data\\nstructures to eﬃciently conduct this search between the query\\nembedding and all document embeddings across the full collec-\\ntion. For this, we employ an oﬀ-the-shelf library for large-scale\\nvector-similarity search, namely faiss [15] from Facebook.4In par-\\nticular, at the end of oﬄine indexing ( §3.4), we maintain a mapping\\nfrom each embedding to its document of origin and then index all\\ndocument embeddings into faiss.\\nSubsequently, when serving queries, we use a two-stage pro-\\ncedure to retrieve the top-k documents from the entire collection.\\nBoth stages rely on ColBERT’s scoring: the /f_irst is an approximate\\nstage aimed at /f_iltering while the second is a re/f_inement stage. For\\nthe /f_irst stage, we concurrently issueNq vector-similarity queries\\n(corresponding to each of the embeddings inEq) onto our faiss in-\\ndex. /T_his retrieves the top-k′(e.g., k′= k/2) matches for that vector\\n4h/t_tps://github.com/facebookresearch/faiss'),\n",
       " Document(metadata={'producer': 'macOS Version 13.5 (Build 22G74) Quartz PDFContext, AppendMode 1.1', 'creator': 'LaTeX with hyperref package', 'creationdate': \"D:20200605003220Z00'00'\", 'author': 'Omar Khattab and Matei Zaharia', 'subject': '', 'trapped': '/False', 'title': 'ColBERT: Efficient and Effective Passage Search via Contextualized Late Interaction over BERT', 'moddate': \"D:20241127020339Z00'00'\", 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'keywords': '', 'source': '/Users/shivamkumar/Documents/Krish-Naik-Course-Agentic AI/agentic-ai-course/data/colbert.pdf', 'total_pages': 10, 'page': 5, 'page_label': '6'}, page_content='over all document embeddings. We map each of those to its docu-\\nment of origin, producing Nq ×k′document IDs, only K ≤Nq ×k′\\nof which are unique. /T_heseK documents likely contain one or more\\nembeddings that are highly similar to the query embeddings. For\\nthe second stage, we re/f_ine this set by exhaustively re-rankingonly\\nthose K documents in the usual manner described in §3.5.\\nIn our faiss-based implementation, we use an IVFPQ index (“in-\\nverted /f_ile with product quantization”). /T_his index partitions the\\nembedding space into P (e.g., P = 1000) cells based onk-means clus-\\ntering and then assigns each document embedding to its nearest cell\\nbased on the selected vector-similarity metric. For serving queries,\\nwhen searching for the top-k′matches for a single query embed-\\nding, only the nearest p (e.g., p = 10) partitions are searched. To\\nimprove memory eﬃciency, every embedding is divided into s (e.g.,\\ns = 16) sub-vectors, each represented using one byte. Moreover,\\nthe index conducts the similarity computations in this compressed\\ndomain, leading to cheaper computations and thus faster search.\\n4 EXPERIMENTAL EVALUATION\\nWe now turn our a/t_tention to empirically testing ColBERT, address-\\ning the following research questions.\\nRQ1: In a typical re-ranking setup, how well can ColBERT bridge\\nthe existing gap (highlighted in §1) between highly-eﬃcient and\\nhighly-eﬀective neural models? (§4.2)\\nRQ2: Beyond re-ranking, can ColBERT eﬀectively support end-\\nto-end retrieval directly from a large collection? (§4.3)\\nRQ3: What does each component of ColBERT (e.g., late interac-\\ntion, query augmentation) contribute to its quality? (§4.4)\\nRQ4: What are ColBERT’s indexing-related costs in terms of\\noﬄine computation and memory overhead? ( §4.5)\\n4.1 Methodology\\n4.1.1 Datasets & Metrics. Similar to related work [ 2, 27, 28],\\nwe conduct our experiments on the MS MARCO Ranking [ 24]\\n(henceforth, MS MARCO) and TREC Complex Answer Retrieval\\n(TREC-CAR) [6] datasets. Both of these recent datasets provide\\nlarge training data of the scale that facilitates training and evaluat-\\ning deep neural networks. We describe both in detail below.\\nMS MARCO.MS MARCO is a dataset (and a corresponding\\ncompetition) introduced by Microso/f_t in 2016 for reading compre-\\nhension and adapted in 2018 for retrieval. It is a collection of 8.8M\\npassages from Web pages, which were gathered from Bing’s results\\nto 1M real-world queries. Each query is associated with sparse\\nrelevance judgements of one (or very few) documents marked as\\nrelevant and no documents explicitly indicated as irrelevant. Per\\nthe oﬃcial evaluation, we use MRR@10 to measure eﬀectiveness.\\nWe use three sets of queries for evaluation. /T_he oﬃcial devel-\\nopment and evaluation sets contain roughly 7k queries. However,\\nthe relevance judgements of the evaluation set are held-out by Mi-\\ncroso/f_t and eﬀectiveness results can only be obtained by submi/t_ting\\nto the competition’s organizers. We submi/t_ted our main re-ranking\\nColBERT model for the results in §4.2. In addition, the collection\\nincludes roughly 55k queries (with labels) that are provided as ad-\\nditional validation data. We re-purpose a random sample of 5k\\nqueries among those (i.e., ones not in our development or training\\nsets) as a “local” evaluation set. Along with the oﬃcial develop-\\nment set, we use this held-out set for testing our models as well as\\nbaselines in §4.3. We do so to avoid submi/t_ting multiple variants\\nof the same model at once, as the organizers discourage too many\\nsubmissions by the same team.\\nTREC CAR.Introduced by Dietz [6] et al.in 2017, TREC CAR\\nis a synthetic dataset based on Wikipedia that consists of about\\n29M passages. Similar to related work [25], we use the /f_irst four of\\n/f_ive pre-de/f_ined folds for training and the /f_i/f_th for validation. /T_his\\namounts to roughly 3M queries generated by concatenating the\\ntitle of a Wikipedia page with the heading of one of its sections.\\n/T_hat section’s passages are marked as relevant to the corresponding\\nquery. Our evaluation is conducted on the test set used in TREC\\n2017 CAR, which contains 2,254 queries.\\n4.1.2 Implementation. Our ColBERT models are implemented\\nusing Python 3 and PyTorch 1. We use the populartransformers5\\nlibrary for the pre-trained BERT model. Similar to [25], we /f_ine-tune\\nall ColBERT models with learning rate 3 ×10−6 with a batch size\\n32. We /f_ix the number of embeddings per query atNq = 32. We set\\nour ColBERT embedding dimension m to be 128; §4.5 demonstrates\\nColBERT’s robustness to a wide range of embedding dimensions.\\nFor MS MARCO, we initialize the BERT components of the Col-\\nBERT query and document encoders using Google’s oﬃcial pre-\\ntrained BERTbase model. Further, we train all models for 200k itera-\\ntions. For TREC CAR, we follow related work [2, 25] and use a dif-\\nferent pre-trained model to the oﬃcial ones. To explain, the oﬃcial\\nBERT models were pre-trained on Wikipedia, which is the source\\nof TREC CAR’s training and test sets. To avoid leaking test data\\ninto train, Nogueira and Cho’s [25] pre-train a randomly-initialized\\nBERT model on the Wiki pages corresponding to training subset of\\nTREC CAR. /T_hey release their BERTlarge pre-trained model, which\\nwe /f_ine-tune for ColBERT’s experiments on TREC CAR. Since /f_ine-\\ntuning this model is signi/f_icantly slower than BERTbase, we train\\non TREC CAR for only 125k iterations.\\nIn our re-ranking results, unless stated otherwise, we use 4 bytes\\nper dimension in our embeddings and employ cosine as our vector-\\nsimilarity function. For end-to-end ranking, we use (squared) L2\\ndistance, as we found our faiss index was faster at L2-based re-\\ntrieval. For our faiss index, we set the number of partitions to\\nP =2,000, and search the nearestp = 10 to each query embedding to\\nretrieve k′= k = 1000 document vectors per query embedding. We\\ndivide each embedding into s = 16 sub-vectors, each encoded using\\none byte. To represent the index used for the second stage of our\\nend-to-end retrieval procedure, we use 16-bit values per dimension.\\n4.1.3 Hardware & Time Measurements. To evaluate the latency\\nof neural re-ranking models in§4.2, we use a single Tesla V100 GPU\\nthat has 32 GiBs of memory on a server with two Intel Xeon Gold\\n6132 CPUs, each with 14 physical cores (24 hyperthreads), and 469\\nGiBs of RAM. For the mostly CPU-based retrieval experiments in\\n§4.3 and the indexing experiments in §4.5, we use another server\\nwith the same CPU and system memory speci/f_ications but which\\nhas four Titan V GPUs a/t_tached, each with 12 GiBs of memory.\\nAcross all experiments, only one GPU is dedicated per query for\\n5h/t_tps://github.com/huggingface/transformers'),\n",
       " Document(metadata={'producer': 'macOS Version 13.5 (Build 22G74) Quartz PDFContext, AppendMode 1.1', 'creator': 'LaTeX with hyperref package', 'creationdate': \"D:20200605003220Z00'00'\", 'author': 'Omar Khattab and Matei Zaharia', 'subject': '', 'trapped': '/False', 'title': 'ColBERT: Efficient and Effective Passage Search via Contextualized Late Interaction over BERT', 'moddate': \"D:20241127020339Z00'00'\", 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'keywords': '', 'source': '/Users/shivamkumar/Documents/Krish-Naik-Course-Agentic AI/agentic-ai-course/data/colbert.pdf', 'total_pages': 10, 'page': 6, 'page_label': '7'}, page_content='Method MRR@10 (Dev) MRR@10 (Eval) Re-ranking Latency (ms) FLOPs/query\\nBM25 (oﬃcial) 16.7 16.5 - -\\nKNRM 19.8 19.8 3 592M (0.085 ×)\\nDuet 24.3 24.5 22 159B (23 ×)\\nfastText+ConvKNRM 29.0 27.7 28 78B (11 ×)\\nBERTbase [25] 34.7 - 10,700 97T (13,900 ×)\\nBERTbase (our training) 36.0 - 10,700 97T (13,900 ×)\\nBERTlarge [25] 36.5 35.9 32,900 340T (48,600 ×)\\nColBERT (over BERTbase) 34.9 34.9 61 7B (1 ×)\\nTable 1: “Re-ranking” results on MS MARCO. Each neural model re-ranks the oﬃcial top-1000 results produced by BM25.\\nLatency is reported for re-ranking only. To obtain the end-to-end latency in Figure 1, we add the BM25 latency from Table 2.\\nMethod MRR@10 (Dev) MRR@10 (Local Eval) Latency (ms) Recall@50 Recall@200 Recall@1000\\nBM25 (oﬃcial) 16.7 - - - - 81.4\\nBM25 (Anserini) 18.7 19.5 62 59.2 73.8 85.7\\ndoc2query 21.5 22.8 85 64.4 77.9 89.1\\nDeepCT 24.3 - 62 (est.) 69 [2] 82 [2] 91 [2]\\ndocTTTTTquery 27.7 28.4 87 75.6 86.9 94.7\\nColBERTL2 (re-rank) 34.8 36.4 - 75.3 80.5 81.4\\nColBERTL2 (end-to-end) 36.0 36.7 458 82.9 92.3 96.8\\nTable 2: End-to-end retrieval results on MS MARCO. Each model retrieves the top-1000 documents per querydirectly from the\\nentire 8.8M document collection.\\nretrieval (i.e., for methods with neural computations) but we use\\nup to all four GPUs during indexing.\\n4.2 /Q_uality–Cost Tradeoﬀ: Top-k Re-ranking\\nIn this section, we examine ColBERT’s eﬃciency and eﬀectiveness\\nat re-ranking the top-k results extracted by a bag-of-words retrieval\\nmodel, which is the most typical se/t_ting for testing and deploying\\nneural ranking models. We begin with the MS MARCO dataset. We\\ncompare against KNRM, Duet, and fastText+ConvKNRM, a repre-\\nsentative set of neural matching models that have been previously\\ntested on MS MARCO. In addition, we compare against the nat-\\nural adaptation of BERT for ranking by Nogueira and Cho [ 25],\\nin particular, BERTbase and its deeper counterpart BERTlarge. We\\nalso report results for “BERTbase (our training)”, which is based on\\nNogueira and Cho’s base model (including hyperparameters) but\\nis trained with the same loss function as ColBERT (§3.3) for 200k\\niterations, allowing for a more direct comparison of the results.\\nWe report the competition’s oﬃcial metric, namely MRR@10,\\non the validation set (Dev) and the evaluation set (Eval). We also\\nreport the re-ranking latency, which we measure using a single\\nTesla V100 GPU, and the FLOPs per query for each neural ranking\\nmodel. For ColBERT, our reported latency subsumes the entire\\ncomputation from gathering the document representations, moving\\nthem to the GPU, tokenizing then encoding the query, and applying\\nlate interaction to compute document scores. For the baselines,\\nwe measure the scoring computations on the GPU and exclude\\nthe CPU-based text preprocessing (similar to [ 9]). In principle,\\nthe baselines can pre-compute the majority of this preprocessing\\n(e.g., document tokenization) oﬄine and parallelize the rest across\\ndocuments online, leaving only a negligible cost. We estimate the\\nFLOPs per query of each model using the torchpro/f_ile6 library.\\nWe now proceed to study the results, which are reported in Ta-\\nble 1. To begin with, we notice the fast progress from KNRM in\\n2017 to the BERT-based models in 2019, manifesting itself in over\\n16% increase in MRR@10. As described in §1, the simultaneous\\nincrease in computational cost is diﬃcult to miss. Judging by their\\nrather monotonic pa/t_tern of increasingly larger cost and higher ef-\\nfectiveness, these results appear to paint a picture where expensive\\nmodels are necessary for high-quality ranking.\\nIn contrast with this trend, ColBERT (which employs late inter-\\naction over BERTbase) performs no worse than the original adap-\\ntation of BERTbase for ranking by Nogueira and Cho [25, 27] and\\nis only marginally less eﬀective than BERT large and our training\\nof BERTbase (described above). While highly competitive in eﬀec-\\ntiveness, ColBERT is orders of magnitude cheaper than BERTbase,\\nin particular, by over 170×in latency and 13,900×in FLOPs. /T_his\\nhighlights the expressiveness of our proposed late interaction mech-\\nanism, particularly when coupled with a powerful pre-trained LM\\nlike BERT. While ColBERT’s re-ranking latency is slightly higher\\nthan the non-BERT re-ranking models shown (i.e., by 10s of mil-\\nliseconds), this diﬀerence is explained by the time it takes to gather,\\nstack, and transfer the document embeddings to the GPU. In partic-\\nular, the query encoding and interaction in ColBERT consume only\\n13 milliseconds of its total execution time. We note that ColBERT’s\\nlatency and FLOPs can be considerably reduced by padding queries\\nto a shorter length, using smaller vector dimensions (the MRR@10\\nof which is tested in §4.5), employing quantization of the document\\n6h/t_tps://github.com/mit-han-lab/torchpro/f_ile'),\n",
       " Document(metadata={'producer': 'macOS Version 13.5 (Build 22G74) Quartz PDFContext, AppendMode 1.1', 'creator': 'LaTeX with hyperref package', 'creationdate': \"D:20200605003220Z00'00'\", 'author': 'Omar Khattab and Matei Zaharia', 'subject': '', 'trapped': '/False', 'title': 'ColBERT: Efficient and Effective Passage Search via Contextualized Late Interaction over BERT', 'moddate': \"D:20241127020339Z00'00'\", 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'keywords': '', 'source': '/Users/shivamkumar/Documents/Krish-Naik-Course-Agentic AI/agentic-ai-course/data/colbert.pdf', 'total_pages': 10, 'page': 7, 'page_label': '8'}, page_content='vectors, and storing the embeddings on GPU if suﬃcient memory\\nexists. We leave these directions for future work.\\n0.27 0.29 0.31 0.33 0.35 0.37\\nMRR@10\\n103\\n104\\n105\\n106\\n107\\n108\\n109\\nMillion FLOPs (log-scale)\\nk=10\\n20\\n50\\n100 200\\n500 1000\\n2000\\nk=10 20 50 100\\n200\\n500\\n1000\\n2000\\nBERTbase (our training)\\nColBERT\\nFigure 4: FLOPs (in millions) and MRR@10 as functions\\nof the re-ranking depthk. Since the oﬃcial BM25 ranking\\nis not ordered, the initial top-k retrieval is conducted with\\nAnserini’s BM25.\\nDiving deeper into the quality–cost tradeoﬀ between BERT and\\nColBERT, Figure 4 demonstrates the relationships between FLOPs\\nand eﬀectiveness (MRR@10) as a function of the re-ranking depth\\nk when re-ranking the top-k results by BM25, comparing ColBERT\\nand BERTbase (our training). We conduct this experiment on MS\\nMARCO (Dev). We note here that as the oﬃcial top-1000 ranking\\ndoes not provide the BM25 order (and also lacks documents beyond\\nthe top-1000 per query), the models in this experiment re-rank the\\nAnserini [37] toolkit’s BM25 output. Consequently, both MRR@10\\nvalues atk = 1000 are slightly higher from those reported in Table 1.\\nStudying the results in Figure 4, we notice that not only is Col-\\nBERT much cheaper than BERT for the same model size (i.e., 12-\\nlayer “base” transformer encoder), it also scales be/t_ter with the\\nnumber of ranked documents. In part, this is because ColBERT\\nonly needs to process the query once, irrespective of the number of\\ndocuments evaluated. For instance, at k = 10, BERT requires nearly\\n180×more FLOPs than ColBERT; at k = 1000, BERT’s overhead\\njumps to 13,900×. It then reaches 23,000×at k = 2000. In fact, our\\ninformal experimentation shows that this orders-of-magnitude gap\\nin FLOPs makes it practical to run ColBERT entirely on the CPU,\\nalthough CPU-based re-ranking lies outside our scope.\\nMethod MAP MRR@10\\nBM25 (Anserini) 15.3 -\\ndoc2query 18.1 -\\nDeepCT 24.6 33.2\\nBM25 + BERTbase 31.0 -\\nBM25 + BERTlarge 33.5 -\\nBM25 + ColBERT 31.3 44.3\\nTable 3: Results on TREC CAR.\\nHaving studied our results on MS MARCO, we now consider\\nTREC CAR, whose oﬃcial metric is MAP. Results are summarized\\nin Table 3, which includes a number of important baselines (BM25,\\ndoc2query, and DeepCT) in addition to re-ranking baselines that\\nhave been tested on this dataset. /T_hese results directly mirror those\\nwith MS MARCO.\\n4.3 End-to-end Top-k Retrieval\\nBeyond cheap re-ranking, ColBERT is amenable to top-k retrieval di-\\nrectly from a full collection. Table 2 considers full retrieval, wherein\\neach model retrieves the top-1000 documents directly from MS\\nMARCO’s 8.8M documents per query. In addition to MRR@10 and\\nlatency in milliseconds, the table reports Recall@50, Recall@200,\\nand Recall@1000, important metrics for a full-retrieval model that\\nessentially /f_ilters down a large collection on a per-query basis.\\nWe compare against BM25, in particular MS MARCO’s oﬃcial\\nBM25 ranking as well as a well-tuned baseline based on the Anserini\\ntoolkit.7 While many other traditional models exist, we are not\\naware of any that substantially outperform Anserini’s BM25 im-\\nplementation (e.g., see RM3 in [28], LMDir in [2], or Microso/f_t’s\\nproprietary feature-based RankSVM on the leaderboard).\\nWe also compare against doc2query, DeepCT, and docTTTT-\\nTquery. All three rely on a traditional bag-of-words model (pri-\\nmarily BM25) for retrieval. Crucially, however, they re-weigh the\\nfrequency of terms per document and/or expand the set of terms\\nin each document before building the BM25 index. In particular,\\ndoc2query expands each document with a pre-de/f_ined number\\nof synthetic queries generated by a seq2seq transformer model\\n(which docTTTTquery replaced with a pre-trained language model,\\nT5 [31]). In contrast, DeepCT uses BERT to produce the term fre-\\nquency component of BM25 in a context-aware manner.\\nFor the latency of Anserini’s BM25, doc2query, and docTTTT-\\nquery, we use the authors’ [26, 28] Anserini-based implementation.\\nWhile this implementation supports multi-threading, it only utilizes\\nparallelism across diﬀerent queries. We thus report single-threaded\\nlatency for these models, noting that simply parallelizing their\\ncomputation over shards of the index can substantially decrease\\ntheir already-low latency. For DeepCT, we only estimate its latency\\nusing that of BM25 (as denoted by (est.) in the table), since DeepCT\\nre-weighs BM25’s term frequency without modifying the index\\notherwise.8 As discussed in §4.1, we use ColBERT L2 for end-to-\\nend retrieval, which employs negative squared L2 distance as its\\nvector-similarity function. For its latency, we measure the time for\\nfaiss-based candidate /f_iltering and the subsequent re-ranking. In\\nthis experiment, faiss uses all available CPU cores.\\nLooking at Table 2, we /f_irst see Anserini’s BM25 baseline at 18.7\\nMRR@10, noticing its very low latency as implemented in Anserini\\n(which extends the well-known Lucene system), owing to both\\nvery cheap operations and decades of bag-of-words top-k retrieval\\noptimizations. /T_he three subsequent baselines, namely doc2query,\\nDeepCT, and docTTTTquery, each brings a decisive enhancement\\nto eﬀectiveness. /T_hese improvements come at negligible overheads\\nin latency, since these baselines ultimately rely on BM25-based\\nretrieval. /T_he most eﬀective among these three, docTTTTquery,\\ndemonstrates a massive 9% gain over vanilla BM25 by /f_ine-tuning\\nthe recent language model T5.\\n7h/t_tp://anserini.io/\\n8In practice, a myriad of reasons could still cause DeepCT’s latency to diﬀer\\nslightly from BM25’s. For instance, the top-k pruning strategy employed, if any, could\\ninteract diﬀerently with a changed distribution of scores.'),\n",
       " Document(metadata={'producer': 'macOS Version 13.5 (Build 22G74) Quartz PDFContext, AppendMode 1.1', 'creator': 'LaTeX with hyperref package', 'creationdate': \"D:20200605003220Z00'00'\", 'author': 'Omar Khattab and Matei Zaharia', 'subject': '', 'trapped': '/False', 'title': 'ColBERT: Efficient and Effective Passage Search via Contextualized Late Interaction over BERT', 'moddate': \"D:20241127020339Z00'00'\", 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'keywords': '', 'source': '/Users/shivamkumar/Documents/Krish-Naik-Course-Agentic AI/agentic-ai-course/data/colbert.pdf', 'total_pages': 10, 'page': 8, 'page_label': '9'}, page_content='Shi/f_ting our a/t_tention to ColBERT’s end-to-end retrieval eﬀec-\\ntiveness, we see its major gains in MRR@10 over all of these end-to-\\nend models. In fact, using ColBERT in the end-to-end setup is supe-\\nrior in terms of MRR@10 to re-ranking with the same model due\\nto the improved recall. Moving beyond MRR@10, we also see large\\ngains in Recall@k for k equals to 50, 200, and 1000. For instance,\\nits Recall@50 actually exceeds the oﬃcial BM25’s Recall@1000 and\\neven all but docTTTTTquery’s Recall@200, emphasizing the value\\nof end-to-end retrieval (instead of just re-ranking) with ColBERT.\\n4.4 Ablation Studies\\n0.22 0.24 0.26 0.28 0.30 0.32 0.34 0.36\\nMRR@10\\nBERT [CLS]-based dot-product (5-layer)  [A]\\nColBERT via average similarity (5-layer)  [B]\\nColBERT without query augmentation (5-layer)  [C]\\nColBERT (5-layer)  [D]\\nColBERT (12-layer)  [E]\\nColBERT + e2e retrieval (12-layer)  [F]\\nFigure 5: Ablation results on MS MARCO (Dev). Between\\nbrackets is the number of BERT layers used in each model.\\n/T_he results from§4.2 indicate that ColBERT is highly eﬀective\\ndespite the low cost and simplicity of its late interaction mechanism.\\nTo be/t_ter understand the source of this eﬀectiveness, we examine a\\nnumber of important details in ColBERT’s interaction and encoder\\narchitecture. For this ablation, we report MRR@10 on the validation\\nset of MS MARCO in Figure 5, which shows our main re-ranking\\nColBERT model [E], with MRR@10 of 34.9%.\\nDue to the cost of training all models, we train a copy of our\\nmain model that retains only the /f_irst 5 layers of BERT out of 12\\n(i.e., model [D]) and similarly train all our ablation models for 200k\\niterations with /f_ive BERT layers. To begin with, we ask if the /f_ine-\\ngranular interaction in late interaction is necessary. Model [A]\\ntackles this question: it uses BERT to produce a single embedding\\nvector for the query and another for the document, extracted from\\nBERT’s [CLS] contextualized embedding and expanded through a\\nlinear layer to dimension 4096 (which equals Nq ×128 = 32 ×128).\\nRelevance is estimated as the inner product of the query’s and the\\ndocument’s embeddings, which we found to perform be/t_ter than\\ncosine similarity for single-vector re-ranking. As the results show,\\nthis model is considerably less eﬀective than ColBERT, reinforcing\\nthe importance of late interaction.\\nSubsequently, we ask if our MaxSim-based late interaction is bet-\\nter than other simple alternatives. We test a model [B] that replaces\\nColBERT’s maximum similarity withaverage similarity. /T_he results\\nsuggest the importance of individual terms in the query paying\\nspecial a/t_tention to particular terms in the document. Similarly,\\nthe /f_igure emphasizes the importance of our query augmentation\\nmechanism: without query augmentation [C], ColBERT has a no-\\nticeably lower MRR@10. Lastly, we see the impact of end-to-end\\nretrieval not only on recall but also on MRR@10. By retrieving\\ndirectly from the full collection, ColBERT is able to retrieve to the\\ntop-10 documents missed entirely from BM25’s top-1000.\\n0 10000 20000 30000 40000 50000\\nThroughput (documents/minute)\\nBasic ColBERT Indexing\\n+multi-GPU document processing\\n+per-batch maximum sequence length\\n+length-based bucketing\\n+multi-core pre-processing\\nFigure 6: Eﬀect of ColBERT’s indexing optimizations on the\\noﬀline indexing throughput.\\n4.5 Indexing /T_hroughput & Footprint\\nLastly, we examine the indexing throughput and space footprint\\nof ColBERT. Figure 6 reports indexing throughput on MS MARCO\\ndocuments with ColBERT and four other ablation se/t_tings, which\\nindividually enable optimizations described in §3.4 on top of basic\\nbatched indexing. Based on these throughputs, ColBERT can index\\nMS MARCO in about three hours. Note that any BERT-based model\\nmust incur the computational cost of processing each document\\nat least once. While ColBERT encodes each document with BERT\\nexactly once, existing BERT-based rankers would repeat similar\\ncomputations on possibly hundreds of documents for each query.\\nSe/t_ting Dimension( m) Bytes/Dim Space(GiBs) MRR@10\\nRe-rank Cosine 128 4 286 34.9\\nEnd-to-end L2 128 2 154 36.0\\nRe-rank L2 128 2 143 34.8\\nRe-rank Cosine 48 4 54 34.4\\nRe-rank Cosine 24 2 27 33.9\\nTable 4: Space Footprint vs MRR@10 (Dev) on MS MARCO.\\nTable 4 reports the space footprint of ColBERT under various\\nse/t_tings as we reduce the embeddings dimension and/or the bytes\\nper dimension. Interestingly, the most space-eﬃcient se/t_ting, that\\nis, re-ranking with cosine similarity with 24-dimensional vectors\\nstored as 2-byte /f_loats, is only 1% worse in MRR@10 than the most\\nspace-consuming one, while the former requires only 27 GiBs to\\nrepresent the MS MARCO collection.\\n5 CONCLUSIONS\\nIn this paper, we introduced ColBERT, a novel ranking model that\\nemploys contextualized late interactionover deep LMs (in particular,\\nBERT) for eﬃcient retrieval. By independently encoding queries\\nand documents into /f_ine-grained representations that interact via\\ncheap and pruning-friendly computations, ColBERT can leverage\\nthe expressiveness of deep LMs while greatly speeding up query\\nprocessing. In addition, doing so allows using ColBERT for end-to-\\nend neural retrieval directly from a large document collection. Our\\nresults show that ColBERT is more than 170×faster and requires\\n14,000×fewer FLOPs/query than existing BERT-based models, all\\nwhile only minimally impacting quality and while outperforming\\nevery non-BERT baseline.\\nAcknowledgments. OK was supported by the Eltoukhy Family\\nGraduate Fellowship at the Stanford School of Engineering. /T_his\\nresearch was supported in part by aﬃliate members and other\\nsupporters of the Stanford DAWN project—Ant Financial, Facebook,\\nGoogle, Infosys, NEC, and VMware—as well as Cisco, SAP, and the'),\n",
       " Document(metadata={'producer': 'macOS Version 13.5 (Build 22G74) Quartz PDFContext, AppendMode 1.1', 'creator': 'LaTeX with hyperref package', 'creationdate': \"D:20200605003220Z00'00'\", 'author': 'Omar Khattab and Matei Zaharia', 'subject': '', 'trapped': '/False', 'title': 'ColBERT: Efficient and Effective Passage Search via Contextualized Late Interaction over BERT', 'moddate': \"D:20241127020339Z00'00'\", 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'keywords': '', 'source': '/Users/shivamkumar/Documents/Krish-Naik-Course-Agentic AI/agentic-ai-course/data/colbert.pdf', 'total_pages': 10, 'page': 9, 'page_label': '10'}, page_content='NSF under CAREER grant CNS-1651570. Any opinions, /f_indings,\\nand conclusions or recommendations expressed in this material are\\nthose of the authors and do not necessarily re/f_lect the views of the\\nNational Science Foundation.\\nREFERENCES\\n[1] Firas Abuzaid, Geet Sethi, Peter Bailis, and Matei Zaharia. 2019. To Index or Not\\nto Index: Optimizing Exact Maximum Inner Product Search. In 2019 IEEE 35th\\nInternational Conference on Data Engineering (ICDE). IEEE, 1250–1261.\\n[2] Zhuyun Dai and Jamie Callan. 2019. Context-Aware Sentence/Passage Term\\nImportance Estimation For First Stage Retrieval. arXiv preprint arXiv:1910.10687\\n(2019).\\n[3] Zhuyun Dai and Jamie Callan. 2019. Deeper Text Understanding for IR with\\nContextual Neural Language Modeling. arXiv preprint arXiv:1905.09217(2019).\\n[4] Zhuyun Dai, Chenyan Xiong, Jamie Callan, and Zhiyuan Liu. 2018. Convolutional\\nneural networks for so/f_t-matching n-grams in ad-hoc search. InProceedings of the\\neleventh ACM international conference on web search and data mining. 126–134.\\n[5] Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. 2018. Bert:\\nPre-training of deep bidirectional transformers for language understanding.\\narXiv preprint arXiv:1810.04805(2018).\\n[6] Laura Dietz, Manisha Verma, Filip Radlinski, and Nick Craswell. 2017. TREC\\nComplex Answer Retrieval Overview.. In TREC.\\n[7] Jiafeng Guo, Yixing Fan, Qingyao Ai, and W Bruce Cro/f_t. 2016. A deep relevance\\nmatching model for ad-hoc retrieval. InProceedings of the 25th ACM International\\non Conference on Information and Knowledge Management. ACM, 55–64.\\n[8] Jiafeng Guo, Yixing Fan, Liang Pang, Liu Yang, Qingyao Ai, Hamed Zamani,\\nChen Wu, W Bruce Cro/f_t, and Xueqi Cheng. 2019. A deep look into neural\\nranking models for information retrieval. arXiv preprint arXiv:1903.06902(2019).\\n[9] Sebastian Hofst¨a/t_ter and Allan Hanbury. 2019. Let’s measure run time! Extending\\nthe IR replicability infrastructure to include performance aspects. arXiv preprint\\narXiv:1907.04614 (2019).\\n[10] Sebastian Hofst¨a/t_ter, Navid Rekabsaz, Carsten Eickhoﬀ, and Allan Hanbury.\\n2019. On the eﬀect of low-frequency terms on neural-IR models. In Proceedings\\nof the 42nd International ACM SIGIR Conference on Research and Development in\\nInformation Retrieval. 1137–1140.\\n[11] Sebastian Hofst¨a/t_ter, Markus Zlabinger, and Allan Hanbury. 2019. TU Wien@\\nTREC Deep Learning’19–Simple Contextualization for Re-ranking.arXiv preprint\\narXiv:1912.01385 (2019).\\n[12] Po-Sen Huang, Xiaodong He, Jianfeng Gao, Li Deng, Alex Acero, and Larry\\nHeck. 2013. Learning deep structured semantic models for web search using\\nclickthrough data. In Proceedings of the 22nd ACM international conference on\\nInformation & Knowledge Management. 2333–2338.\\n[13] Shiyu Ji, Jinjin Shao, and Tao Yang. 2019. Eﬃcient Interaction-based Neural\\nRanking with Locality Sensitive Hashing. In /T_he World Wide Web Conference.\\nACM, 2858–2864.\\n[14] Xiaoqi Jiao, Yichun Yin, Lifeng Shang, Xin Jiang, Xiao Chen, Linlin Li, Fang Wang,\\nand /Q_un Liu. 2019. Tinybert: Distilling bert for natural language understanding.\\narXiv preprint arXiv:1909.10351(2019).\\n[15] Jeﬀ Johnson, Ma/t_thijs Douze, and Herv´e J´egou. 2017. Billion-scale similarity\\nsearch with GPUs. arXiv preprint arXiv:1702.08734(2017).\\n[16] Diederik P Kingma and Jimmy Ba. 2014. Adam: A method for stochastic opti-\\nmization. arXiv preprint arXiv:1412.6980(2014).\\n[17] Ron Kohavi, Alex Deng, Brian Frasca, Toby Walker, Ya Xu, and Nils Pohlmann.\\n2013. Online controlled experiments at large scale. In SIGKDD.\\n[18] Sean MacAvaney, Andrew Yates, Arman Cohan, and Nazli Goharian. 2019. Cedr:\\nContextualized embeddings for document ranking. In Proceedings of the 42nd\\nInternational ACM SIGIR Conference on Research and Development in Information\\nRetrieval. ACM, 1101–1104.\\n[19] Paul Michel, Omer Levy, and Graham Neubig. 2019. Are Sixteen Heads Really\\nBe/t_ter than One?. InAdvances in Neural Information Processing Systems. 14014–\\n14024.\\n[20] Bhaskar Mitra and Nick Craswell. 2019. An Updated Duet Model for Passage\\nRe-ranking. arXiv preprint arXiv:1903.07666(2019).\\n[21] Bhaskar Mitra, Nick Craswell, et al. 2018. An introduction to neural information\\nretrieval. Foundations and Trends® in Information Retrieval13, 1 (2018), 1–126.\\n[22] Bhaskar Mitra, Fernando Diaz, and Nick Craswell. 2017. Learning to match using\\nlocal and distributed representations of text for web search. In Proceedings of\\nthe 26th International Conference on World Wide Web. International World Wide\\nWeb Conferences Steering Commi/t_tee, 1291–1299.\\n[23] Bhaskar Mitra, Corby Rosset, David Hawking, Nick Craswell, Fernando Diaz,\\nand Emine Yilmaz. 2019. Incorporating query term independence assumption\\nfor eﬃcient retrieval and ranking using deep neural networks. arXiv preprint\\narXiv:1907.03693 (2019).\\n[24] Tri Nguyen, Mir Rosenberg, Xia Song, Jianfeng Gao, Saurabh Tiwary, Rangan\\nMajumder, and Li Deng. 2016. MS MARCO: A Human-Generated MAchine\\nReading COmprehension Dataset. (2016).\\n[25] Rodrigo Nogueira and Kyunghyun Cho. 2019. Passage Re-ranking with BERT.\\narXiv preprint arXiv:1901.04085(2019).\\n[26] Rodrigo Nogueira, Jimmy Lin, and AI Epistemic. 2019. From doc2query to\\ndocTTTTTquery. (2019).\\n[27] Rodrigo Nogueira, Wei Yang, Kyunghyun Cho, and Jimmy Lin. 2019. Multi-Stage\\nDocument Ranking with BERT. arXiv preprint arXiv:1910.14424(2019).\\n[28] Rodrigo Nogueira, Wei Yang, Jimmy Lin, and Kyunghyun Cho. 2019. Document\\nExpansion by /Q_uery Prediction.arXiv preprint arXiv:1904.08375(2019).\\n[29] Ma/t_thew E Peters, Mark Neumann, Mohit Iyyer, Ma/t_t Gardner, Christopher\\nClark, Kenton Lee, and Luke Ze/t_tlemoyer. 2018. Deep contextualized word\\nrepresentations. arXiv preprint arXiv:1802.05365(2018).\\n[30] Yifan Qiao, Chenyan Xiong, Zhenghao Liu, and Zhiyuan Liu. 2019. Under-\\nstanding the Behaviors of BERT in Ranking. arXiv preprint arXiv:1904.07531\\n(2019).\\n[31] Colin Raﬀel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang,\\nMichael Matena, Yanqi Zhou, Wei Li, and Peter J Liu. 2019. Exploring the\\nlimits of transfer learning with a uni/f_ied text-to-text transformer.arXiv preprint\\narXiv:1910.10683 (2019).\\n[32] Stephen E Robertson, Steve Walker, Susan Jones, Micheline M Hancock-Beaulieu,\\nMike Gatford, et al. 1995. Okapi at TREC-3. NIST Special Publication(1995).\\n[33] Raphael Tang, Yao Lu, Linqing Liu, Lili Mou, Olga Vechtomova, and Jimmy Lin.\\n2019. Distilling task-speci/f_ic knowledge from BERT into simple neural networks.\\narXiv preprint arXiv:1903.12136(2019).\\n[34] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones,\\nAidan N Gomez,  Lukasz Kaiser, and Illia Polosukhin. 2017. A/t_tention is all\\nyou need. In Advances in neural information processing systems. 5998–6008.\\n[35] Yonghui Wu, Mike Schuster, Zhifeng Chen, /Q_uoc V Le, Mohammad Norouzi,\\nWolfgang Macherey, Maxim Krikun, Yuan Cao, Qin Gao, Klaus Macherey, et al.\\n2016. Google’s neural machine translation system: Bridging the gap between\\nhuman and machine translation. arXiv preprint arXiv:1609.08144(2016).\\n[36] Chenyan Xiong, Zhuyun Dai, Jamie Callan, Zhiyuan Liu, and Russell Power.\\n2017. End-to-end neural ad-hoc ranking with kernel pooling. In Proceedings\\nof the 40th International ACM SIGIR conference on research and development in\\ninformation retrieval. 55–64.\\n[37] Peilin Yang, Hui Fang, and Jimmy Lin. 2018. Anserini: Reproducible ranking\\nbaselines using Lucene. Journal of Data and Information /Q_uality (JDIQ)10, 4\\n(2018), 1–20.\\n[38] Wei Yang, Kuang Lu, Peilin Yang, and Jimmy Lin. 2019. Critically Examining\\nthe” Neural Hype” Weak Baselines and the Additivity of Eﬀectiveness Gains\\nfrom Neural Ranking Models. In Proceedings of the 42nd International ACM SIGIR\\nConference on Research and Development in Information Retrieval. 1129–1132.\\n[39] Zeynep Akkalyoncu Yilmaz, Wei Yang, Haotian Zhang, and Jimmy Lin. 2019.\\nCross-domain modeling of sentence-level evidence for document retrieval. In\\nProceedings of the 2019 Conference on Empirical Methods in Natural Language Pro-\\ncessing and the 9th International Joint Conference on Natural Language Processing\\n(EMNLP-IJCNLP). 3481–3487.\\n[40] O/f_ir Zafrir, Guy Boudoukh, Peter Izsak, and Moshe Wasserblat. 2019. Q8bert:\\n/Q_uantized 8bit bert.arXiv preprint arXiv:1910.06188(2019).\\n[41] Hamed Zamani, Mostafa Dehghani, W Bruce Cro/f_t, Erik Learned-Miller, and\\nJaap Kamps. 2018. From neural re-ranking to neural ranking: Learning a sparse\\nrepresentation for inverted indexing. InProceedings of the 27th ACM International\\nConference on Information and Knowledge Management. ACM, 497–506.\\n[42] Le Zhao. 2012. Modeling and solving term mismatch for full-text retrieval. Ph.D.\\nDissertation. Carnegie Mellon University.')]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b1972014",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'ColBERT: Eﬀicient and Eﬀective Passage Search via\\nContextualized Late Interaction over BERT\\nOmar Kha/t_tab\\nStanford University\\nokha/t_tab@stanford.edu\\nMatei Zaharia\\nStanford University\\nmatei@cs.stanford.edu\\nABSTRACT\\nRecent progress in Natural Language Understanding (NLU) is driv-\\ning fast-paced advances in Information Retrieval (IR), largely owed\\nto /f_ine-tuning deep language models (LMs) for document ranking.\\nWhile remarkably eﬀective, the ranking models based on these LMs\\nincrease computational cost by orders of magnitude over prior ap-\\nproaches, particularly as they must feed each query–document pair\\nthrough a massive neural network to compute a single relevance\\nscore. To tackle this, we present ColBERT, a novel ranking model\\nthat adapts deep LMs (in particular, BERT) for eﬃcient retrieval.\\nColBERT introduces a late interactionarchitecture that indepen-\\ndently encodes the query and the document using BERT and then\\nemploys a cheap yet powerful interaction step that models their\\n/f_ine-grained similarity. By delaying and yet retaining this /f_ine-\\ngranular interaction, ColBERT can leverage the expressiveness of\\ndeep LMs while simultaneously gaining the ability to pre-compute\\ndocument representations oﬄine, considerably speeding up query\\nprocessing. Beyond reducing the cost of re-ranking the documents\\nretrieved by a traditional model, ColBERT’s pruning-friendly in-\\nteraction mechanism enables leveraging vector-similarity indexes\\nfor end-to-end retrieval directly from a large document collection.\\nWe extensively evaluate ColBERT using two recent passage search\\ndatasets. Results show that ColBERT’s eﬀectiveness is competitive\\nwith existing BERT-based models (and outperforms every non-\\nBERT baseline), while executing two orders-of-magnitude faster\\nand requiring four orders-of-magnitude fewer FLOPs per query.\\nACM Reference format:\\nOmar Kha/t_tab and Matei Zaharia. 2020. ColBERT: Eﬃcient and Eﬀective Pas-\\nsage Search via Contextualized Late Interaction over BERT. In Proceedings\\nof Proceedings of the 43rd International ACM SIGIR Conference on Research\\nand Development in Information Retrieval, Virtual Event, China, July 25–30,\\n2020 (SIGIR ’20),10 pages.\\nDOI: 10.1145/3397271.3401075\\n1 INTRODUCTION\\nOver the past few years, the Information Retrieval (IR) community\\nhas witnessed the introduction of a host of neural ranking models,\\nincluding DRMM [7], KNRM [4, 36], and Duet [20, 22]. In contrast\\nPermission to make digital or hard copies of all or part of this work for personal or\\nclassroom use is granted without fee provided that copies are not made or distributed\\nfor pro/f_it or commercial advantage and that copies bear this notice and the full citation\\non the /f_irst page. Copyrights for components of this work owned by others than the\\nauthor(s) must be honored. Abstracting with credit is permi/t_ted. To copy otherwise, or\\nrepublish, to post on servers or to redistribute to lists, requires prior speci/f_ic permission\\nand/or a fee. Request permissions from permissions@acm.org.\\nSIGIR ’20, Virtual Event, China\\n© 2020 Copyright held by the owner/author(s). Publication rights licensed to ACM.\\n978-1-4503-8016-4/20/07. . .$15.00\\nDOI: 10.1145/3397271.3401075\\n0.15 0.20 0.25 0.30 0.35 0.40\\nMRR@10\\n101\\n102\\n103\\n104\\n105\\nQuery Latency (ms)\\nBM25 doc2queryKNRM\\nDuet\\nDeepCT\\nfT+ConvKNRM\\ndocTTTTTquery\\nBERT-base\\nBERT-large\\nColBERT (re-rank)\\nColBERT (full retrieval)\\nBag-of-Words (BoW) Model\\nBoW Model with NLU Augmentation\\nNeural Matching Model\\nDeep Language Model\\nColBERT (ours)\\nFigure 1: Eﬀectiveness (MRR@10) versus Mean /Q_uery La-\\ntency (log-scale) for a number of representative ranking\\nmodels on MS MARCO Ranking [24]. /T_he /f_igure also shows\\nColBERT. Neural re-rankers run on top of the oﬃcial BM25\\ntop-1000 results and use a Tesla V100 GPU. Methodology and\\ndetailed results are in§4.\\nto prior learning-to-rank methods that rely on hand-cra/f_ted fea-\\ntures, these models employ embedding-based representations of\\nqueries and documents and directly model local interactions(i.e.,\\n/f_ine-granular relationships) between their contents. Among them,\\na recent approach has emerged that /f_ine-tunesdeep pre-trained\\nlanguage models (LMs) like ELMo [29] and BERT [5] for estimating\\nrelevance. By computing deeply-contextualized semantic repre-\\nsentations of query–document pairs, these LMs help bridge the\\npervasive vocabulary mismatch [21, 42] between documents and\\nqueries [30]. Indeed, in the span of just a few months, a number\\nof ranking models based on BERT have achieved state-of-the-art\\nresults on various retrieval benchmarks [ 3, 18, 25, 39] and have\\nbeen proprietarily adapted for deployment by Google1 and Bing2.\\nHowever, the remarkable gains delivered by these LMs come\\nat a steep increase in computational cost. Hofst¨a/t_teret al.[9] and\\nMacAvaney et al.[18] observe that BERT-based models in the lit-\\nerature are 100-1000×more computationally expensive than prior\\nmodels—some of which are arguably not inexpensive to begin with\\n[13]. /T_his quality–cost tradeoﬀ is summarized by Figure 1, which\\ncompares two BERT-based rankers [25, 27] against a representative\\nset of ranking models. /T_he /f_igure uses MS MARCO Ranking [24],\\na recent collection of 9M passages and 1M queries from Bing’s\\nlogs. It reports retrieval eﬀectiveness (MRR@10) on the oﬃcial\\nvalidation set as well as average query latency (log-scale) using a\\nhigh-end server that dedicates one Tesla V100 GPU per query for\\nneural re-rankers. Following the re-ranking setup of MS MARCO,\\nColBERT (re-rank), the Neural Matching Models, and the Deep LMs\\nre-rank the MS MARCO’s oﬃcial top-1000 documents per query.\\n1h/t_tps://blog.google/products/search/search-language-understanding-bert/\\n2h/t_tps://azure.microso/f_t.com/en-us/blog/bing-delivers-its-largest-improvement-\\nin-search-experience-using-azure-gpus/\\narXiv:2004.12832v2  [cs.IR]  4 Jun 2020'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "documents[0].page_content\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "dd5bb543",
   "metadata": {},
   "outputs": [],
   "source": [
    "doc_string=[doc.page_content for doc in documents]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "38f6779f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(doc_string)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "aad897cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting chromadb\n",
      "  Downloading chromadb-1.0.13-cp39-abi3-macosx_11_0_arm64.whl (17.9 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m17.9/17.9 MB\u001b[0m \u001b[31m1.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0mm\n",
      "\u001b[?25hCollecting build>=1.0.3 (from chromadb)\n",
      "  Using cached build-1.2.2.post1-py3-none-any.whl (22 kB)\n",
      "Requirement already satisfied: pydantic>=1.9 in /Users/shivamkumar/Documents/Krish-Naik-Course-Agentic AI/agentic-ai-course/venv/lib/python3.11/site-packages (from chromadb) (2.11.7)\n",
      "Collecting pybase64>=1.4.1 (from chromadb)\n",
      "  Downloading pybase64-1.4.1-cp311-cp311-macosx_11_0_arm64.whl (31 kB)\n",
      "Collecting uvicorn[standard]>=0.18.3 (from chromadb)\n",
      "  Downloading uvicorn-0.34.3-py3-none-any.whl (62 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.4/62.4 kB\u001b[0m \u001b[31m607.0 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: numpy>=1.22.5 in /Users/shivamkumar/Documents/Krish-Naik-Course-Agentic AI/agentic-ai-course/venv/lib/python3.11/site-packages (from chromadb) (2.3.0)\n",
      "Collecting posthog>=2.4.0 (from chromadb)\n",
      "  Downloading posthog-5.1.0-py3-none-any.whl (103 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m103.4/103.4 kB\u001b[0m \u001b[31m4.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: typing-extensions>=4.5.0 in /Users/shivamkumar/Documents/Krish-Naik-Course-Agentic AI/agentic-ai-course/venv/lib/python3.11/site-packages (from chromadb) (4.14.0)\n",
      "Collecting onnxruntime>=1.14.1 (from chromadb)\n",
      "  Downloading onnxruntime-1.22.0-cp311-cp311-macosx_13_0_universal2.whl (34.3 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m34.3/34.3 MB\u001b[0m \u001b[31m1.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0mm\n",
      "\u001b[?25hCollecting opentelemetry-api>=1.2.0 (from chromadb)\n",
      "  Downloading opentelemetry_api-1.34.1-py3-none-any.whl (65 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m65.8/65.8 kB\u001b[0m \u001b[31m2.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hCollecting opentelemetry-exporter-otlp-proto-grpc>=1.2.0 (from chromadb)\n",
      "  Downloading opentelemetry_exporter_otlp_proto_grpc-1.34.1-py3-none-any.whl (18 kB)\n",
      "Collecting opentelemetry-sdk>=1.2.0 (from chromadb)\n",
      "  Downloading opentelemetry_sdk-1.34.1-py3-none-any.whl (118 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m118.5/118.5 kB\u001b[0m \u001b[31m2.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: tokenizers>=0.13.2 in /Users/shivamkumar/Documents/Krish-Naik-Course-Agentic AI/agentic-ai-course/venv/lib/python3.11/site-packages (from chromadb) (0.21.1)\n",
      "Collecting pypika>=0.48.9 (from chromadb)\n",
      "  Using cached PyPika-0.48.9-py2.py3-none-any.whl\n",
      "Requirement already satisfied: tqdm>=4.65.0 in /Users/shivamkumar/Documents/Krish-Naik-Course-Agentic AI/agentic-ai-course/venv/lib/python3.11/site-packages (from chromadb) (4.67.1)\n",
      "Collecting overrides>=7.3.1 (from chromadb)\n",
      "  Using cached overrides-7.7.0-py3-none-any.whl (17 kB)\n",
      "Collecting importlib-resources (from chromadb)\n",
      "  Using cached importlib_resources-6.5.2-py3-none-any.whl (37 kB)\n",
      "Requirement already satisfied: grpcio>=1.58.0 in /Users/shivamkumar/Documents/Krish-Naik-Course-Agentic AI/agentic-ai-course/venv/lib/python3.11/site-packages (from chromadb) (1.73.0)\n",
      "Collecting bcrypt>=4.0.1 (from chromadb)\n",
      "  Downloading bcrypt-4.3.0-cp39-abi3-macosx_10_12_universal2.whl (498 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m499.0/499.0 kB\u001b[0m \u001b[31m1.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting typer>=0.9.0 (from chromadb)\n",
      "  Downloading typer-0.16.0-py3-none-any.whl (46 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m46.3/46.3 kB\u001b[0m \u001b[31m3.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting kubernetes>=28.1.0 (from chromadb)\n",
      "  Downloading kubernetes-33.1.0-py2.py3-none-any.whl (1.9 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.9/1.9 MB\u001b[0m \u001b[31m1.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m0m\n",
      "\u001b[?25hRequirement already satisfied: tenacity>=8.2.3 in /Users/shivamkumar/Documents/Krish-Naik-Course-Agentic AI/agentic-ai-course/venv/lib/python3.11/site-packages (from chromadb) (9.1.2)\n",
      "Requirement already satisfied: pyyaml>=6.0.0 in /Users/shivamkumar/Documents/Krish-Naik-Course-Agentic AI/agentic-ai-course/venv/lib/python3.11/site-packages (from chromadb) (6.0.2)\n",
      "Collecting mmh3>=4.0.1 (from chromadb)\n",
      "  Downloading mmh3-5.1.0-cp311-cp311-macosx_11_0_arm64.whl (40 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m40.1/40.1 kB\u001b[0m \u001b[31m4.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: orjson>=3.9.12 in /Users/shivamkumar/Documents/Krish-Naik-Course-Agentic AI/agentic-ai-course/venv/lib/python3.11/site-packages (from chromadb) (3.10.18)\n",
      "Requirement already satisfied: httpx>=0.27.0 in /Users/shivamkumar/Documents/Krish-Naik-Course-Agentic AI/agentic-ai-course/venv/lib/python3.11/site-packages (from chromadb) (0.28.1)\n",
      "Collecting rich>=10.11.0 (from chromadb)\n",
      "  Using cached rich-14.0.0-py3-none-any.whl (243 kB)\n",
      "Collecting jsonschema>=4.19.0 (from chromadb)\n",
      "  Downloading jsonschema-4.24.0-py3-none-any.whl (88 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m88.7/88.7 kB\u001b[0m \u001b[31m3.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: packaging>=19.1 in /Users/shivamkumar/Documents/Krish-Naik-Course-Agentic AI/agentic-ai-course/venv/lib/python3.11/site-packages (from build>=1.0.3->chromadb) (24.2)\n",
      "Collecting pyproject_hooks (from build>=1.0.3->chromadb)\n",
      "  Using cached pyproject_hooks-1.2.0-py3-none-any.whl (10 kB)\n",
      "Requirement already satisfied: anyio in /Users/shivamkumar/Documents/Krish-Naik-Course-Agentic AI/agentic-ai-course/venv/lib/python3.11/site-packages (from httpx>=0.27.0->chromadb) (4.9.0)\n",
      "Requirement already satisfied: certifi in /Users/shivamkumar/Documents/Krish-Naik-Course-Agentic AI/agentic-ai-course/venv/lib/python3.11/site-packages (from httpx>=0.27.0->chromadb) (2025.6.15)\n",
      "Requirement already satisfied: httpcore==1.* in /Users/shivamkumar/Documents/Krish-Naik-Course-Agentic AI/agentic-ai-course/venv/lib/python3.11/site-packages (from httpx>=0.27.0->chromadb) (1.0.9)\n",
      "Requirement already satisfied: idna in /Users/shivamkumar/Documents/Krish-Naik-Course-Agentic AI/agentic-ai-course/venv/lib/python3.11/site-packages (from httpx>=0.27.0->chromadb) (3.10)\n",
      "Requirement already satisfied: h11>=0.16 in /Users/shivamkumar/Documents/Krish-Naik-Course-Agentic AI/agentic-ai-course/venv/lib/python3.11/site-packages (from httpcore==1.*->httpx>=0.27.0->chromadb) (0.16.0)\n",
      "Requirement already satisfied: attrs>=22.2.0 in /Users/shivamkumar/Documents/Krish-Naik-Course-Agentic AI/agentic-ai-course/venv/lib/python3.11/site-packages (from jsonschema>=4.19.0->chromadb) (25.3.0)\n",
      "Collecting jsonschema-specifications>=2023.03.6 (from jsonschema>=4.19.0->chromadb)\n",
      "  Downloading jsonschema_specifications-2025.4.1-py3-none-any.whl (18 kB)\n",
      "Collecting referencing>=0.28.4 (from jsonschema>=4.19.0->chromadb)\n",
      "  Downloading referencing-0.36.2-py3-none-any.whl (26 kB)\n",
      "Collecting rpds-py>=0.7.1 (from jsonschema>=4.19.0->chromadb)\n",
      "  Downloading rpds_py-0.25.1-cp311-cp311-macosx_11_0_arm64.whl (359 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m359.1/359.1 kB\u001b[0m \u001b[31m952.7 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: six>=1.9.0 in /Users/shivamkumar/Documents/Krish-Naik-Course-Agentic AI/agentic-ai-course/venv/lib/python3.11/site-packages (from kubernetes>=28.1.0->chromadb) (1.17.0)\n",
      "Requirement already satisfied: python-dateutil>=2.5.3 in /Users/shivamkumar/Documents/Krish-Naik-Course-Agentic AI/agentic-ai-course/venv/lib/python3.11/site-packages (from kubernetes>=28.1.0->chromadb) (2.9.0.post0)\n",
      "Requirement already satisfied: google-auth>=1.0.1 in /Users/shivamkumar/Documents/Krish-Naik-Course-Agentic AI/agentic-ai-course/venv/lib/python3.11/site-packages (from kubernetes>=28.1.0->chromadb) (2.40.3)\n",
      "Collecting websocket-client!=0.40.0,!=0.41.*,!=0.42.*,>=0.32.0 (from kubernetes>=28.1.0->chromadb)\n",
      "  Using cached websocket_client-1.8.0-py3-none-any.whl (58 kB)\n",
      "Requirement already satisfied: requests in /Users/shivamkumar/Documents/Krish-Naik-Course-Agentic AI/agentic-ai-course/venv/lib/python3.11/site-packages (from kubernetes>=28.1.0->chromadb) (2.32.4)\n",
      "Collecting requests-oauthlib (from kubernetes>=28.1.0->chromadb)\n",
      "  Using cached requests_oauthlib-2.0.0-py2.py3-none-any.whl (24 kB)\n",
      "Collecting oauthlib>=3.2.2 (from kubernetes>=28.1.0->chromadb)\n",
      "  Downloading oauthlib-3.3.0-py3-none-any.whl (165 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m165.2/165.2 kB\u001b[0m \u001b[31m1.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: urllib3>=1.24.2 in /Users/shivamkumar/Documents/Krish-Naik-Course-Agentic AI/agentic-ai-course/venv/lib/python3.11/site-packages (from kubernetes>=28.1.0->chromadb) (2.4.0)\n",
      "Collecting durationpy>=0.7 (from kubernetes>=28.1.0->chromadb)\n",
      "  Downloading durationpy-0.10-py3-none-any.whl (3.9 kB)\n",
      "Collecting coloredlogs (from onnxruntime>=1.14.1->chromadb)\n",
      "  Using cached coloredlogs-15.0.1-py2.py3-none-any.whl (46 kB)\n",
      "Collecting flatbuffers (from onnxruntime>=1.14.1->chromadb)\n",
      "  Downloading flatbuffers-25.2.10-py2.py3-none-any.whl (30 kB)\n",
      "Requirement already satisfied: protobuf in /Users/shivamkumar/Documents/Krish-Naik-Course-Agentic AI/agentic-ai-course/venv/lib/python3.11/site-packages (from onnxruntime>=1.14.1->chromadb) (6.31.1)\n",
      "Requirement already satisfied: sympy in /Users/shivamkumar/Documents/Krish-Naik-Course-Agentic AI/agentic-ai-course/venv/lib/python3.11/site-packages (from onnxruntime>=1.14.1->chromadb) (1.14.0)\n",
      "Collecting importlib-metadata<8.8.0,>=6.0 (from opentelemetry-api>=1.2.0->chromadb)\n",
      "  Downloading importlib_metadata-8.7.0-py3-none-any.whl (27 kB)\n",
      "Requirement already satisfied: googleapis-common-protos~=1.52 in /Users/shivamkumar/Documents/Krish-Naik-Course-Agentic AI/agentic-ai-course/venv/lib/python3.11/site-packages (from opentelemetry-exporter-otlp-proto-grpc>=1.2.0->chromadb) (1.70.0)\n",
      "Collecting opentelemetry-exporter-otlp-proto-common==1.34.1 (from opentelemetry-exporter-otlp-proto-grpc>=1.2.0->chromadb)\n",
      "  Downloading opentelemetry_exporter_otlp_proto_common-1.34.1-py3-none-any.whl (18 kB)\n",
      "Collecting opentelemetry-proto==1.34.1 (from opentelemetry-exporter-otlp-proto-grpc>=1.2.0->chromadb)\n",
      "  Downloading opentelemetry_proto-1.34.1-py3-none-any.whl (55 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m55.7/55.7 kB\u001b[0m \u001b[31m504.3 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hCollecting protobuf (from onnxruntime>=1.14.1->chromadb)\n",
      "  Using cached protobuf-5.29.5-cp38-abi3-macosx_10_9_universal2.whl (418 kB)\n",
      "Collecting opentelemetry-semantic-conventions==0.55b1 (from opentelemetry-sdk>=1.2.0->chromadb)\n",
      "  Downloading opentelemetry_semantic_conventions-0.55b1-py3-none-any.whl (196 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m196.2/196.2 kB\u001b[0m \u001b[31m1.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hCollecting backoff>=1.10.0 (from posthog>=2.4.0->chromadb)\n",
      "  Using cached backoff-2.2.1-py3-none-any.whl (15 kB)\n",
      "Requirement already satisfied: distro>=1.5.0 in /Users/shivamkumar/Documents/Krish-Naik-Course-Agentic AI/agentic-ai-course/venv/lib/python3.11/site-packages (from posthog>=2.4.0->chromadb) (1.9.0)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in /Users/shivamkumar/Documents/Krish-Naik-Course-Agentic AI/agentic-ai-course/venv/lib/python3.11/site-packages (from pydantic>=1.9->chromadb) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.33.2 in /Users/shivamkumar/Documents/Krish-Naik-Course-Agentic AI/agentic-ai-course/venv/lib/python3.11/site-packages (from pydantic>=1.9->chromadb) (2.33.2)\n",
      "Requirement already satisfied: typing-inspection>=0.4.0 in /Users/shivamkumar/Documents/Krish-Naik-Course-Agentic AI/agentic-ai-course/venv/lib/python3.11/site-packages (from pydantic>=1.9->chromadb) (0.4.1)\n",
      "Collecting markdown-it-py>=2.2.0 (from rich>=10.11.0->chromadb)\n",
      "  Using cached markdown_it_py-3.0.0-py3-none-any.whl (87 kB)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /Users/shivamkumar/Documents/Krish-Naik-Course-Agentic AI/agentic-ai-course/venv/lib/python3.11/site-packages (from rich>=10.11.0->chromadb) (2.19.1)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.16.4 in /Users/shivamkumar/Documents/Krish-Naik-Course-Agentic AI/agentic-ai-course/venv/lib/python3.11/site-packages (from tokenizers>=0.13.2->chromadb) (0.33.0)\n",
      "Collecting click>=8.0.0 (from typer>=0.9.0->chromadb)\n",
      "  Downloading click-8.2.1-py3-none-any.whl (102 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m102.2/102.2 kB\u001b[0m \u001b[31m4.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting shellingham>=1.3.0 (from typer>=0.9.0->chromadb)\n",
      "  Using cached shellingham-1.5.4-py2.py3-none-any.whl (9.8 kB)\n",
      "Collecting httptools>=0.6.3 (from uvicorn[standard]>=0.18.3->chromadb)\n",
      "  Using cached httptools-0.6.4-cp311-cp311-macosx_11_0_arm64.whl (103 kB)\n",
      "Requirement already satisfied: python-dotenv>=0.13 in /Users/shivamkumar/Documents/Krish-Naik-Course-Agentic AI/agentic-ai-course/venv/lib/python3.11/site-packages (from uvicorn[standard]>=0.18.3->chromadb) (1.1.0)\n",
      "Collecting uvloop>=0.15.1 (from uvicorn[standard]>=0.18.3->chromadb)\n",
      "  Using cached uvloop-0.21.0-cp311-cp311-macosx_10_9_universal2.whl (1.4 MB)\n",
      "Collecting watchfiles>=0.13 (from uvicorn[standard]>=0.18.3->chromadb)\n",
      "  Downloading watchfiles-1.1.0-cp311-cp311-macosx_11_0_arm64.whl (397 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m397.3/397.3 kB\u001b[0m \u001b[31m1.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hCollecting websockets>=10.4 (from uvicorn[standard]>=0.18.3->chromadb)\n",
      "  Downloading websockets-15.0.1-cp311-cp311-macosx_11_0_arm64.whl (173 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m173.3/173.3 kB\u001b[0m \u001b[31m1.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: cachetools<6.0,>=2.0.0 in /Users/shivamkumar/Documents/Krish-Naik-Course-Agentic AI/agentic-ai-course/venv/lib/python3.11/site-packages (from google-auth>=1.0.1->kubernetes>=28.1.0->chromadb) (5.5.2)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in /Users/shivamkumar/Documents/Krish-Naik-Course-Agentic AI/agentic-ai-course/venv/lib/python3.11/site-packages (from google-auth>=1.0.1->kubernetes>=28.1.0->chromadb) (0.4.2)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in /Users/shivamkumar/Documents/Krish-Naik-Course-Agentic AI/agentic-ai-course/venv/lib/python3.11/site-packages (from google-auth>=1.0.1->kubernetes>=28.1.0->chromadb) (4.9.1)\n",
      "Requirement already satisfied: filelock in /Users/shivamkumar/Documents/Krish-Naik-Course-Agentic AI/agentic-ai-course/venv/lib/python3.11/site-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers>=0.13.2->chromadb) (3.18.0)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /Users/shivamkumar/Documents/Krish-Naik-Course-Agentic AI/agentic-ai-course/venv/lib/python3.11/site-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers>=0.13.2->chromadb) (2025.5.1)\n",
      "Requirement already satisfied: hf-xet<2.0.0,>=1.1.2 in /Users/shivamkumar/Documents/Krish-Naik-Course-Agentic AI/agentic-ai-course/venv/lib/python3.11/site-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers>=0.13.2->chromadb) (1.1.4)\n",
      "Collecting zipp>=3.20 (from importlib-metadata<8.8.0,>=6.0->opentelemetry-api>=1.2.0->chromadb)\n",
      "  Downloading zipp-3.23.0-py3-none-any.whl (10 kB)\n",
      "Collecting mdurl~=0.1 (from markdown-it-py>=2.2.0->rich>=10.11.0->chromadb)\n",
      "  Using cached mdurl-0.1.2-py3-none-any.whl (10.0 kB)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /Users/shivamkumar/Documents/Krish-Naik-Course-Agentic AI/agentic-ai-course/venv/lib/python3.11/site-packages (from requests->kubernetes>=28.1.0->chromadb) (3.4.2)\n",
      "Requirement already satisfied: sniffio>=1.1 in /Users/shivamkumar/Documents/Krish-Naik-Course-Agentic AI/agentic-ai-course/venv/lib/python3.11/site-packages (from anyio->httpx>=0.27.0->chromadb) (1.3.1)\n",
      "Collecting humanfriendly>=9.1 (from coloredlogs->onnxruntime>=1.14.1->chromadb)\n",
      "  Using cached humanfriendly-10.0-py2.py3-none-any.whl (86 kB)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /Users/shivamkumar/Documents/Krish-Naik-Course-Agentic AI/agentic-ai-course/venv/lib/python3.11/site-packages (from sympy->onnxruntime>=1.14.1->chromadb) (1.3.0)\n",
      "Requirement already satisfied: pyasn1<0.7.0,>=0.6.1 in /Users/shivamkumar/Documents/Krish-Naik-Course-Agentic AI/agentic-ai-course/venv/lib/python3.11/site-packages (from pyasn1-modules>=0.2.1->google-auth>=1.0.1->kubernetes>=28.1.0->chromadb) (0.6.1)\n",
      "Installing collected packages: pypika, flatbuffers, durationpy, zipp, websockets, websocket-client, uvloop, shellingham, rpds-py, pyproject_hooks, pybase64, protobuf, overrides, oauthlib, mmh3, mdurl, importlib-resources, humanfriendly, httptools, click, bcrypt, backoff, watchfiles, uvicorn, requests-oauthlib, referencing, posthog, opentelemetry-proto, markdown-it-py, importlib-metadata, coloredlogs, build, rich, opentelemetry-exporter-otlp-proto-common, opentelemetry-api, onnxruntime, kubernetes, jsonschema-specifications, typer, opentelemetry-semantic-conventions, jsonschema, opentelemetry-sdk, opentelemetry-exporter-otlp-proto-grpc, chromadb\n",
      "  Attempting uninstall: protobuf\n",
      "    Found existing installation: protobuf 6.31.1\n",
      "    Uninstalling protobuf-6.31.1:\n",
      "      Successfully uninstalled protobuf-6.31.1\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "grpcio-status 1.73.0 requires protobuf<7.0.0,>=6.30.0, but you have protobuf 5.29.5 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0mSuccessfully installed backoff-2.2.1 bcrypt-4.3.0 build-1.2.2.post1 chromadb-1.0.13 click-8.2.1 coloredlogs-15.0.1 durationpy-0.10 flatbuffers-25.2.10 httptools-0.6.4 humanfriendly-10.0 importlib-metadata-8.7.0 importlib-resources-6.5.2 jsonschema-4.24.0 jsonschema-specifications-2025.4.1 kubernetes-33.1.0 markdown-it-py-3.0.0 mdurl-0.1.2 mmh3-5.1.0 oauthlib-3.3.0 onnxruntime-1.22.0 opentelemetry-api-1.34.1 opentelemetry-exporter-otlp-proto-common-1.34.1 opentelemetry-exporter-otlp-proto-grpc-1.34.1 opentelemetry-proto-1.34.1 opentelemetry-sdk-1.34.1 opentelemetry-semantic-conventions-0.55b1 overrides-7.7.0 posthog-5.1.0 protobuf-5.29.5 pybase64-1.4.1 pypika-0.48.9 pyproject_hooks-1.2.0 referencing-0.36.2 requests-oauthlib-2.0.0 rich-14.0.0 rpds-py-0.25.1 shellingham-1.5.4 typer-0.16.0 uvicorn-0.34.3 uvloop-0.21.0 watchfiles-1.1.0 websocket-client-1.8.0 websockets-15.0.1 zipp-3.23.0\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.1.2\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.1.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install chromadb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "04bb229b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create vector store for the documents and generate embeddings\n",
    "db=Chroma.from_documents(documents,embeddings)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "e8c292ea",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'title': 'ColBERT: Efficient and Effective Passage Search via Contextualized Late Interaction over BERT', 'page_label': '2', 'moddate': \"D:20241127020339Z00'00'\", 'total_pages': 10, 'subject': '', 'creationdate': \"D:20200605003220Z00'00'\", 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'producer': 'macOS Version 13.5 (Build 22G74) Quartz PDFContext, AppendMode 1.1', 'trapped': '/False', 'author': 'Omar Khattab and Matei Zaharia', 'keywords': '', 'source': '/Users/shivamkumar/Documents/Krish-Naik-Course-Agentic AI/agentic-ai-course/data/colbert.pdf', 'creator': 'LaTeX with hyperref package', 'page': 1}, page_content='QueryDocument\\nMaxSim∑MaxSimMaxSimsQueryCNN  /  Match KernelsCNN  /  Match Kernels / MLPMLPs\\nDocument(c) All-to-all Interaction(e.g., BERT)(b) Query-Document Interaction(e.g., DRMM, KNRM, Conv-KNRM)(d) Late Interaction(i.e., the proposed ColBERT)(a) Representation-based Similarity(e.g., DSSM, SNRM)QueryDocument\\nsQueryDocument\\ns\\nFigure 2: Schematic diagrams illustrating query–document matching paradigms in neural IR. /T_he /f_igure contrasts existing\\napproaches (sub-/f_igures (a), (b), and (c)) with the proposed late interaction paradigm (sub-/f_igure (d)).\\nOther methods, including ColBERT (full retrieval), directly retrieve\\nthe top-1000 results from the entire collection.\\nAs the /f_igure shows, BERT considerably improves search preci-\\nsion, raising MRR@10 by almost 7% against the best previous meth-\\nods; simultaneously, it increases latency by up to tens of thousands\\nof milliseconds even with a high-end GPU. /T_his poses a challenging\\ntradeoﬀ since raising query response times by as li/t_tle as 100ms is\\nknown to impact user experience and even measurably diminish\\nrevenue [17]. To tackle this problem, recent work has started ex-\\nploring using Natural Language Understanding (NLU) techniques\\nto augment traditional retrieval models like BM25 [32]. For exam-\\nple, Nogueira et al.[26, 28] expand documents with NLU-generated\\nqueries before indexing with BM25 scores and Dai & Callan [2] re-\\nplace BM25’s term frequency with NLU-estimated term importance.\\nDespite successfully reducing latency, these approaches generally\\nreduce precision substantially relative to BERT.\\nTo reconcile eﬃciency and contextualization in IR, we propose\\nColBERT, a ranking model based on contextualized late interac-\\ntion over BERT. As the name suggests, ColBERT proposes a novel\\nlate interactionparadigm for estimating relevance between a query\\nq and a document d. Under late interaction, q and d are separately\\nencoded into two sets of contextual embeddings, and relevance is\\nevaluated using cheap and pruning-friendly computations between\\nboth sets—that is, fast computations that enable ranking without\\nexhaustively evaluating every possible candidate.\\nFigure 2 contrasts our proposed late interaction approach with\\nexisting neural matching paradigms. On the le/f_t, Figure 2 (a) illus-\\ntrates representation-focused rankers, which independently compute\\nan embedding for q and another for d and estimate relevance as\\na single similarity score between two vectors [12, 41]. Moving to\\nthe right, Figure 2 (b) visualizes typical interaction-focused rankers.\\nInstead of summarizing q and d into individual embeddings, these\\nrankers model word- and phrase-level relationships across q and d\\nand match them using a deep neural network (e.g., with CNNs/MLPs\\n[22] or kernels [36]). In the simplest case, they feed the neural net-\\nwork an interaction matrix that re/f_lects the similiarity between\\nevery pair of words across q and d. Further right, Figure 2 (c) illus-\\ntrates a more powerful interaction-based paradigm, which models\\nthe interactions between words within as well as across q and d at\\nthe same time, as in BERT’s transformer architecture [25].\\n/T_hese increasingly expressive architectures are in tension. While\\ninteraction-based models (i.e., Figure 2 (b) and (c)) tend to be su-\\nperior for IR tasks [8, 21], a representation-focused model—by iso-\\nlating the computations among q and d—makes it possible to pre-\\ncompute document representations oﬄine [ 41], greatly reducing\\nthe computational load per query. In this work, we observe that\\nthe /f_ine-grained matching of interaction-based models and the pre-\\ncomputation of document representations of representation-based\\nmodels can be combined by retaining yet judiciously delaying the\\nquery–document interaction. Figure 2 (d) illustrates an architec-\\nture that precisely does so. As illustrated, every query embedding\\ninteracts with all document embeddings via a MaxSim operator,\\nwhich computes maximum similarity (e.g., cosine similarity), and\\nthe scalar outputs of these operators are summed across query\\nterms. /T_his paradigm allows ColBERT to exploit deep LM-based\\nrepresentations while shi/f_ting the cost of encoding documents of-\\n/f_line and amortizing the cost of encoding the query once across\\nall ranked documents. Additionally, it enables ColBERT to lever-\\nage vector-similarity search indexes (e.g., [ 1, 15]) to retrieve the\\ntop-k results directly from a large document collection, substan-\\ntially improving recall over models that only re-rank the output of\\nterm-based retrieval.\\nAs Figure 1 illustrates, ColBERT can serve queries in tens or\\nfew hundreds of milliseconds. For instance, when used for re-\\nranking as in “ColBERT (re-rank)”, it delivers over 170×speedup\\n(and requires 14,000×fewer FLOPs) relative to existing BERT-based\\nmodels, while being more eﬀective than every non-BERT baseline\\n(§4.2 & 4.3). ColBERT’s indexing—the only time it needs to feed\\ndocuments through BERT—is also practical: it can index the MS\\nMARCO collection of 9M passages in about 3 hours using a single\\nserver with four GPUs (§4.5), retaining its eﬀectiveness with a space\\nfootprint of as li/t_tle as few tens of GiBs. Our extensive ablation\\nstudy (§4.4) shows that late interaction, its implementation via\\nMaxSim operations, and crucial design choices within our BERT-\\nbased encoders are all essential to ColBERT’s eﬀectiveness.\\nOur main contributions are as follows.\\n(1) We proposelate interaction(§3.1) as a paradigm for eﬃcient\\nand eﬀective neural ranking.\\n(2) We present ColBERT (§3.2 & 3.3), a highly-eﬀective model\\nthat employs novel BERT-based query and document en-\\ncoders within the late interaction paradigm.'),\n",
       " Document(metadata={'subject': '', 'producer': 'macOS Version 13.5 (Build 22G74) Quartz PDFContext, AppendMode 1.1', 'keywords': '', 'title': 'ColBERT: Efficient and Effective Passage Search via Contextualized Late Interaction over BERT', 'page': 6, 'moddate': \"D:20241127020339Z00'00'\", 'trapped': '/False', 'source': '/Users/shivamkumar/Documents/Krish-Naik-Course-Agentic AI/agentic-ai-course/data/colbert.pdf', 'author': 'Omar Khattab and Matei Zaharia', 'creationdate': \"D:20200605003220Z00'00'\", 'creator': 'LaTeX with hyperref package', 'page_label': '7', 'total_pages': 10, 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2'}, page_content='Method MRR@10 (Dev) MRR@10 (Eval) Re-ranking Latency (ms) FLOPs/query\\nBM25 (oﬃcial) 16.7 16.5 - -\\nKNRM 19.8 19.8 3 592M (0.085 ×)\\nDuet 24.3 24.5 22 159B (23 ×)\\nfastText+ConvKNRM 29.0 27.7 28 78B (11 ×)\\nBERTbase [25] 34.7 - 10,700 97T (13,900 ×)\\nBERTbase (our training) 36.0 - 10,700 97T (13,900 ×)\\nBERTlarge [25] 36.5 35.9 32,900 340T (48,600 ×)\\nColBERT (over BERTbase) 34.9 34.9 61 7B (1 ×)\\nTable 1: “Re-ranking” results on MS MARCO. Each neural model re-ranks the oﬃcial top-1000 results produced by BM25.\\nLatency is reported for re-ranking only. To obtain the end-to-end latency in Figure 1, we add the BM25 latency from Table 2.\\nMethod MRR@10 (Dev) MRR@10 (Local Eval) Latency (ms) Recall@50 Recall@200 Recall@1000\\nBM25 (oﬃcial) 16.7 - - - - 81.4\\nBM25 (Anserini) 18.7 19.5 62 59.2 73.8 85.7\\ndoc2query 21.5 22.8 85 64.4 77.9 89.1\\nDeepCT 24.3 - 62 (est.) 69 [2] 82 [2] 91 [2]\\ndocTTTTTquery 27.7 28.4 87 75.6 86.9 94.7\\nColBERTL2 (re-rank) 34.8 36.4 - 75.3 80.5 81.4\\nColBERTL2 (end-to-end) 36.0 36.7 458 82.9 92.3 96.8\\nTable 2: End-to-end retrieval results on MS MARCO. Each model retrieves the top-1000 documents per querydirectly from the\\nentire 8.8M document collection.\\nretrieval (i.e., for methods with neural computations) but we use\\nup to all four GPUs during indexing.\\n4.2 /Q_uality–Cost Tradeoﬀ: Top-k Re-ranking\\nIn this section, we examine ColBERT’s eﬃciency and eﬀectiveness\\nat re-ranking the top-k results extracted by a bag-of-words retrieval\\nmodel, which is the most typical se/t_ting for testing and deploying\\nneural ranking models. We begin with the MS MARCO dataset. We\\ncompare against KNRM, Duet, and fastText+ConvKNRM, a repre-\\nsentative set of neural matching models that have been previously\\ntested on MS MARCO. In addition, we compare against the nat-\\nural adaptation of BERT for ranking by Nogueira and Cho [ 25],\\nin particular, BERTbase and its deeper counterpart BERTlarge. We\\nalso report results for “BERTbase (our training)”, which is based on\\nNogueira and Cho’s base model (including hyperparameters) but\\nis trained with the same loss function as ColBERT (§3.3) for 200k\\niterations, allowing for a more direct comparison of the results.\\nWe report the competition’s oﬃcial metric, namely MRR@10,\\non the validation set (Dev) and the evaluation set (Eval). We also\\nreport the re-ranking latency, which we measure using a single\\nTesla V100 GPU, and the FLOPs per query for each neural ranking\\nmodel. For ColBERT, our reported latency subsumes the entire\\ncomputation from gathering the document representations, moving\\nthem to the GPU, tokenizing then encoding the query, and applying\\nlate interaction to compute document scores. For the baselines,\\nwe measure the scoring computations on the GPU and exclude\\nthe CPU-based text preprocessing (similar to [ 9]). In principle,\\nthe baselines can pre-compute the majority of this preprocessing\\n(e.g., document tokenization) oﬄine and parallelize the rest across\\ndocuments online, leaving only a negligible cost. We estimate the\\nFLOPs per query of each model using the torchpro/f_ile6 library.\\nWe now proceed to study the results, which are reported in Ta-\\nble 1. To begin with, we notice the fast progress from KNRM in\\n2017 to the BERT-based models in 2019, manifesting itself in over\\n16% increase in MRR@10. As described in §1, the simultaneous\\nincrease in computational cost is diﬃcult to miss. Judging by their\\nrather monotonic pa/t_tern of increasingly larger cost and higher ef-\\nfectiveness, these results appear to paint a picture where expensive\\nmodels are necessary for high-quality ranking.\\nIn contrast with this trend, ColBERT (which employs late inter-\\naction over BERTbase) performs no worse than the original adap-\\ntation of BERTbase for ranking by Nogueira and Cho [25, 27] and\\nis only marginally less eﬀective than BERT large and our training\\nof BERTbase (described above). While highly competitive in eﬀec-\\ntiveness, ColBERT is orders of magnitude cheaper than BERTbase,\\nin particular, by over 170×in latency and 13,900×in FLOPs. /T_his\\nhighlights the expressiveness of our proposed late interaction mech-\\nanism, particularly when coupled with a powerful pre-trained LM\\nlike BERT. While ColBERT’s re-ranking latency is slightly higher\\nthan the non-BERT re-ranking models shown (i.e., by 10s of mil-\\nliseconds), this diﬀerence is explained by the time it takes to gather,\\nstack, and transfer the document embeddings to the GPU. In partic-\\nular, the query encoding and interaction in ColBERT consume only\\n13 milliseconds of its total execution time. We note that ColBERT’s\\nlatency and FLOPs can be considerably reduced by padding queries\\nto a shorter length, using smaller vector dimensions (the MRR@10\\nof which is tested in §4.5), employing quantization of the document\\n6h/t_tps://github.com/mit-han-lab/torchpro/f_ile'),\n",
       " Document(metadata={'total_pages': 10, 'creator': 'LaTeX with hyperref package', 'page': 8, 'author': 'Omar Khattab and Matei Zaharia', 'source': '/Users/shivamkumar/Documents/Krish-Naik-Course-Agentic AI/agentic-ai-course/data/colbert.pdf', 'producer': 'macOS Version 13.5 (Build 22G74) Quartz PDFContext, AppendMode 1.1', 'title': 'ColBERT: Efficient and Effective Passage Search via Contextualized Late Interaction over BERT', 'moddate': \"D:20241127020339Z00'00'\", 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'keywords': '', 'page_label': '9', 'subject': '', 'creationdate': \"D:20200605003220Z00'00'\", 'trapped': '/False'}, page_content='Shi/f_ting our a/t_tention to ColBERT’s end-to-end retrieval eﬀec-\\ntiveness, we see its major gains in MRR@10 over all of these end-to-\\nend models. In fact, using ColBERT in the end-to-end setup is supe-\\nrior in terms of MRR@10 to re-ranking with the same model due\\nto the improved recall. Moving beyond MRR@10, we also see large\\ngains in Recall@k for k equals to 50, 200, and 1000. For instance,\\nits Recall@50 actually exceeds the oﬃcial BM25’s Recall@1000 and\\neven all but docTTTTTquery’s Recall@200, emphasizing the value\\nof end-to-end retrieval (instead of just re-ranking) with ColBERT.\\n4.4 Ablation Studies\\n0.22 0.24 0.26 0.28 0.30 0.32 0.34 0.36\\nMRR@10\\nBERT [CLS]-based dot-product (5-layer)  [A]\\nColBERT via average similarity (5-layer)  [B]\\nColBERT without query augmentation (5-layer)  [C]\\nColBERT (5-layer)  [D]\\nColBERT (12-layer)  [E]\\nColBERT + e2e retrieval (12-layer)  [F]\\nFigure 5: Ablation results on MS MARCO (Dev). Between\\nbrackets is the number of BERT layers used in each model.\\n/T_he results from§4.2 indicate that ColBERT is highly eﬀective\\ndespite the low cost and simplicity of its late interaction mechanism.\\nTo be/t_ter understand the source of this eﬀectiveness, we examine a\\nnumber of important details in ColBERT’s interaction and encoder\\narchitecture. For this ablation, we report MRR@10 on the validation\\nset of MS MARCO in Figure 5, which shows our main re-ranking\\nColBERT model [E], with MRR@10 of 34.9%.\\nDue to the cost of training all models, we train a copy of our\\nmain model that retains only the /f_irst 5 layers of BERT out of 12\\n(i.e., model [D]) and similarly train all our ablation models for 200k\\niterations with /f_ive BERT layers. To begin with, we ask if the /f_ine-\\ngranular interaction in late interaction is necessary. Model [A]\\ntackles this question: it uses BERT to produce a single embedding\\nvector for the query and another for the document, extracted from\\nBERT’s [CLS] contextualized embedding and expanded through a\\nlinear layer to dimension 4096 (which equals Nq ×128 = 32 ×128).\\nRelevance is estimated as the inner product of the query’s and the\\ndocument’s embeddings, which we found to perform be/t_ter than\\ncosine similarity for single-vector re-ranking. As the results show,\\nthis model is considerably less eﬀective than ColBERT, reinforcing\\nthe importance of late interaction.\\nSubsequently, we ask if our MaxSim-based late interaction is bet-\\nter than other simple alternatives. We test a model [B] that replaces\\nColBERT’s maximum similarity withaverage similarity. /T_he results\\nsuggest the importance of individual terms in the query paying\\nspecial a/t_tention to particular terms in the document. Similarly,\\nthe /f_igure emphasizes the importance of our query augmentation\\nmechanism: without query augmentation [C], ColBERT has a no-\\nticeably lower MRR@10. Lastly, we see the impact of end-to-end\\nretrieval not only on recall but also on MRR@10. By retrieving\\ndirectly from the full collection, ColBERT is able to retrieve to the\\ntop-10 documents missed entirely from BM25’s top-1000.\\n0 10000 20000 30000 40000 50000\\nThroughput (documents/minute)\\nBasic ColBERT Indexing\\n+multi-GPU document processing\\n+per-batch maximum sequence length\\n+length-based bucketing\\n+multi-core pre-processing\\nFigure 6: Eﬀect of ColBERT’s indexing optimizations on the\\noﬀline indexing throughput.\\n4.5 Indexing /T_hroughput & Footprint\\nLastly, we examine the indexing throughput and space footprint\\nof ColBERT. Figure 6 reports indexing throughput on MS MARCO\\ndocuments with ColBERT and four other ablation se/t_tings, which\\nindividually enable optimizations described in §3.4 on top of basic\\nbatched indexing. Based on these throughputs, ColBERT can index\\nMS MARCO in about three hours. Note that any BERT-based model\\nmust incur the computational cost of processing each document\\nat least once. While ColBERT encodes each document with BERT\\nexactly once, existing BERT-based rankers would repeat similar\\ncomputations on possibly hundreds of documents for each query.\\nSe/t_ting Dimension( m) Bytes/Dim Space(GiBs) MRR@10\\nRe-rank Cosine 128 4 286 34.9\\nEnd-to-end L2 128 2 154 36.0\\nRe-rank L2 128 2 143 34.8\\nRe-rank Cosine 48 4 54 34.4\\nRe-rank Cosine 24 2 27 33.9\\nTable 4: Space Footprint vs MRR@10 (Dev) on MS MARCO.\\nTable 4 reports the space footprint of ColBERT under various\\nse/t_tings as we reduce the embeddings dimension and/or the bytes\\nper dimension. Interestingly, the most space-eﬃcient se/t_ting, that\\nis, re-ranking with cosine similarity with 24-dimensional vectors\\nstored as 2-byte /f_loats, is only 1% worse in MRR@10 than the most\\nspace-consuming one, while the former requires only 27 GiBs to\\nrepresent the MS MARCO collection.\\n5 CONCLUSIONS\\nIn this paper, we introduced ColBERT, a novel ranking model that\\nemploys contextualized late interactionover deep LMs (in particular,\\nBERT) for eﬃcient retrieval. By independently encoding queries\\nand documents into /f_ine-grained representations that interact via\\ncheap and pruning-friendly computations, ColBERT can leverage\\nthe expressiveness of deep LMs while greatly speeding up query\\nprocessing. In addition, doing so allows using ColBERT for end-to-\\nend neural retrieval directly from a large document collection. Our\\nresults show that ColBERT is more than 170×faster and requires\\n14,000×fewer FLOPs/query than existing BERT-based models, all\\nwhile only minimally impacting quality and while outperforming\\nevery non-BERT baseline.\\nAcknowledgments. OK was supported by the Eltoukhy Family\\nGraduate Fellowship at the Stanford School of Engineering. /T_his\\nresearch was supported in part by aﬃliate members and other\\nsupporters of the Stanford DAWN project—Ant Financial, Facebook,\\nGoogle, Infosys, NEC, and VMware—as well as Cisco, SAP, and the')]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "retriever = db.as_retriever(search_kwargs={\"k\": 3})\n",
    "\n",
    "docs = retriever.invoke(\"Late Interaction\")\n",
    "docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "8020d088",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creation of Pydantic Class \n",
    "\n",
    "# Till now we have created llm, embedding model, document loader, text splitter and vector store.\n",
    "# Now we will create a pydantic class to define the schema of the input and output\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0c7faa9",
   "metadata": {},
   "source": [
    "# Creation of Pydantic class for output parsing "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "e4c23bc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import operator\n",
    "from typing import List\n",
    "from pydantic import BaseModel , Field\n",
    "from langchain.prompts import PromptTemplate\n",
    "from typing import TypedDict, Annotated, Sequence\n",
    "from langchain_core.messages import BaseMessage\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.prompts import ChatPromptTemplate, PromptTemplate\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "from langchain_core.messages import HumanMessage, AIMessage\n",
    "from langgraph.graph import StateGraph,END"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "f552c805",
   "metadata": {},
   "outputs": [],
   "source": [
    "# a) Representation-based Similarity\n",
    "# (e.g., DSSM, SNRM)\n",
    "# Query\n",
    "# (b) Query-Document Interaction\n",
    "# (e.g., DRMM, KNRM, Conv-KNRM)\n",
    "# Query Document\n",
    "# (c) All-to-all Interaction\n",
    "# (e.g., BERT)\n",
    "# Query Document\n",
    "# (d) Late Interaction\n",
    "# (i.e., the proposed ColBERT)\n",
    "class RetrievalTechnique(BaseModel):\n",
    "    technique: str = Field(description=\"The technique used for retrieval, e.g., 'Late Interaction', 'respresentation based similarity'.\")\n",
    "    reasoning: str = Field(description=\"The reasoning behind the choice of retrieval technique, e.g., 'Late Interaction is chosen because it allows for efficient retrieval of relevant documents while maintaining high accuracy.'\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "ec0f83a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.output_parsers import PydanticOutputParser\n",
    "parser = PydanticOutputParser(pydantic_object=RetrievalTechnique)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "9ed3db1e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The output should be formatted as a JSON instance that conforms to the JSON schema below.\\n\\nAs an example, for the schema {\"properties\": {\"foo\": {\"title\": \"Foo\", \"description\": \"a list of strings\", \"type\": \"array\", \"items\": {\"type\": \"string\"}}}, \"required\": [\"foo\"]}\\nthe object {\"foo\": [\"bar\", \"baz\"]} is a well-formatted instance of the schema. The object {\"properties\": {\"foo\": [\"bar\", \"baz\"]}} is not well-formatted.\\n\\nHere is the output schema:\\n```\\n{\"properties\": {\"technique\": {\"description\": \"The technique used for retrieval, e.g., \\'Late Interaction\\', \\'respresentation based similarity\\'.\", \"title\": \"Technique\", \"type\": \"string\"}, \"reasoning\": {\"description\": \"The reasoning behind the choice of retrieval technique, e.g., \\'Late Interaction is chosen because it allows for efficient retrieval of relevant documents while maintaining high accuracy.\\'\", \"title\": \"Reasoning\", \"type\": \"string\"}}, \"required\": [\"technique\", \"reasoning\"]}\\n```'"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "parser.get_format_instructions()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2328568e",
   "metadata": {},
   "source": [
    "# Langgraph AgentState Initialization "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "9450603d",
   "metadata": {},
   "outputs": [],
   "source": [
    "AgentState={}\n",
    "AgentState[\"messages\"] = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "1276a316",
   "metadata": {},
   "outputs": [],
   "source": [
    "AgentState[\"messages\"].append(\"What is the retrieval technique used in ColBERT?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "f0355816",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'messages': ['What is the retrieval technique used in ColBERT?']}"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "AgentState"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "5a097386",
   "metadata": {},
   "outputs": [],
   "source": [
    "AgentState[\"messages\"].append(\"Late interaction is a retrieval technique that allows for efficient retrieval of relevant documents while maintaining high accuracy.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "213f1d47",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'messages': ['What is the retrieval technique used in ColBERT?',\n",
       "  'Late interaction is a retrieval technique that allows for efficient retrieval of relevant documents while maintaining high accuracy.']}"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "AgentState"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "985613f7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Late interaction is a retrieval technique that allows for efficient retrieval of relevant documents while maintaining high accuracy.'"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "AgentState[\"messages\"][-1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad073af3",
   "metadata": {},
   "source": [
    "# AgentState inside the state graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "9c65f564",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AgentState(TypedDict):\n",
    "    messages : Annotated[Sequence[BaseMessage], operator.add]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "ee98639e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "__main__.AgentState"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "AgentState"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "3f2253a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Response(BaseModel):\n",
    "    topic: str = Field(description=\"The classification topic.\")\n",
    "    reasoning: str = Field(description=\"The reasoning behind the classification\")\n",
    "\n",
    "from langchain_core.output_parsers import JsonOutputParser\n",
    "parser = JsonOutputParser(pydantic_object=Response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45bbb1d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def func1(state: AgentState):\n",
    "    question = state[\"messages\"][-1]\n",
    " \n",
    "    template = \"\"\"\n",
    "Your task is to classify the question into one of the following categories:\n",
    "[Sports, Entertainment, Politics, Technology, Health, Education, Business, Science, Environment, Travel, Food, Fashion, Art, History, Literature, Music, Movies, Television, Gaming, Automotive]\n",
    "\n",
    "Note: If the question is not related to any of the above categories, classify it as 'Other'.\n",
    "\n",
    "{format_instructions}\n",
    "\n",
    "Question: {question}\n",
    "\"\"\"\n",
    "\n",
    "    prompt = PromptTemplate(\n",
    "        input_variables=[\"question\"],\n",
    "        template=template,\n",
    "        partial_variables={\"format_instructions\": parser.get_format_instructions()}\n",
    "    )\n",
    "\n",
    "    chain = prompt | model | parser\n",
    "    response = chain.invoke({\"question\": question})\n",
    "    print(response)\n",
    "    \n",
    "    # state[\"classification\"] = response\n",
    "    return state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4c74558",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'category': 'Business'}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'category': 'Business'}"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "state = {\n",
    "    \"messages\": [\"What is the GDP of India?\"]\n",
    "}\n",
    "\n",
    "func1(state)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "8846d0a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'category': 'Food'}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'category': 'Food'}"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "state = {\n",
    "    \"messages\": [\"Chemicals and masala used for making biryani?\"]\n",
    "}\n",
    "\n",
    "func1(state)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "a0feb64e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def func2(state: AgentState):\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "94f265d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def func3(state: AgentState):\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "5dc2119d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def router(state: AgentState):\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03d4a48f",
   "metadata": {},
   "source": [
    "# Creation of StateGraph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "34731dbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langgraph.graph import StateGraph, END"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "06ef4efc",
   "metadata": {},
   "outputs": [],
   "source": [
    "workflow = StateGraph(AgentState) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "e8ed7678",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<langgraph.graph.state.StateGraph at 0x31d31d250>"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "workflow.add_node(\"Supervisior\", func1)\n",
    "workflow.add_node(\"LLM\", func2)\n",
    "workflow.add_node(\"RAG\", func3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "201fd821",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<langgraph.graph.state.StateGraph at 0x31d31d250>"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "workflow.set_entry_point(\"Supervisior\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "e3c15a73",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<langgraph.graph.state.StateGraph at 0x31d31d250>"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# conditional edges, node1 name , function which will be called, and the end node name \n",
    "# so router should return the name of next node to  be called\n",
    "\n",
    "workflow.add_conditional_edges(\n",
    "    \"Supervisior\", router , {\"LLM_Call\" : \"LLM\" , \"RAG_Call\" : \"RAG\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "e2a4f445",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<langgraph.graph.state.StateGraph at 0x31d31d250>"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "workflow.add_edge(\"LLM\", END)\n",
    "workflow.add_edge(\"RAG\", END)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "639ace73",
   "metadata": {},
   "outputs": [],
   "source": [
    "app = workflow.compile()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "3d0ea023",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.display import Image, display\n",
    "display(Image(app.get_graph().draw_mermaid_png()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75f33eb8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
